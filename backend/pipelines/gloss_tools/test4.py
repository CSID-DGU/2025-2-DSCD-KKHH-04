# -*- coding: utf-8 -*-
"""
1) gloss_dictionary_MOCK_1.csvì˜ korean_meanings ì—´ì—ì„œ ëª¨ë“  í•œêµ­ì–´ ë‹¨ì–´ ëª©ë¡ ìˆ˜ì§‘
2) gloss_tokens.txtì˜ ë‹¨ì–´ê°€ ì‚¬ì „ì— ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
3) ì‚¬ì „ì— ì—†ëŠ” ë‹¨ì–´ë“¤ì— ëŒ€í•´ substring/split/fuzzy ê¸°ë°˜ í›„ë³´ë¥¼ ìƒì„±í•˜ì—¬ JSON ì €ì¥
"""

import csv
import ast
import unicodedata
import re
import json
from pathlib import Path
from difflib import get_close_matches  # fuzzy ê²€ìƒ‰

ROOT_DIR = Path(__file__).resolve().parent

DICT_CSV_PATH   = ROOT_DIR / "gloss_dictionary_MOCK_1.csv"
GLOSS_TOKENS    = ROOT_DIR / "gloss_tokens.txt"
MISSING_JSON    = ROOT_DIR / "gloss_missing_map4.json"


def norm(s: str) -> str:
    """ì „ê°/ë°˜ê° í†µì¼ + ê³µë°± ì •ê·œí™”."""
    s = unicodedata.normalize("NFKC", s or "").strip()
    return re.sub(r"\s+", " ", s)


def load_korean_terms_from_dict(csv_path: Path) -> set[str]:
    """CSV ì‚¬ì „ì—ì„œ korean_meanings ê³„ì—´ ì»¬ëŸ¼ì˜ í•œêµ­ì–´ ë‹¨ì–´ë“¤ì„ setìœ¼ë¡œ ë¡œë“œ."""
    terms = set()

    with open(csv_path, "r", encoding="utf-8-sig") as f:
        rdr = csv.DictReader(f)
        headers = [h.lower().strip() for h in (rdr.fieldnames or [])]

        cand = [
            "korean_meanings", "korean", "ko",
            "meaning_ko", "ko_meanings", "korean_meaning",
        ]
        h_ko = next((c for c in cand if c in headers), None)

        if not h_ko:
            raise RuntimeError(f"korean_meanings ì—´ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í—¤ë”: {headers}")

        for row in rdr:
            cell = (row.get(h_ko) or "").strip()
            if not cell:
                continue

            try:
                obj = ast.literal_eval(cell)
                if isinstance(obj, (list, tuple)):
                    items = [str(x) for x in obj]
                else:
                    items = [str(obj)]
            except Exception:
                items = [cell]

            for t in items:
                t_norm = norm(t)
                if t_norm:
                    terms.add(t_norm)

    return terms


# -------------------------------
# ë¸Œëœë“œ/ìƒí’ˆëª… ìŠ¤íƒ€ì¼ íŒë³„ (ê³ ìœ ì–´ ì¶”ì •)
# -------------------------------

def is_brand_like(tok: str) -> bool:
    """
    KBìŠ¤ë§ˆíŠ¸ë¼ì´í”„í”ŒëŸ¬ìŠ¤ ê°™ì€ 'ìƒí’ˆëª…/ë¸Œëœë“œëª…' ìŠ¤íƒ€ì¼ í† í°ì„ ëŒ€ëµì ìœ¼ë¡œ íŒë³„.
    - ì˜ë¬¸ì/ìˆ«ì í¬í•¨
    - KB/êµ­ë¯¼/ì‹ í•œ/ìš°ë¦¬/í•˜ë‚˜ ë“±ì˜ ì€í–‰ í‚¤ì›Œë“œ í¬í•¨
    - ê¸¸ì´ê°€ ê½¤ ê¸¸ê³  ê³µë°± ì—†ì´ ë¶™ì–´ ìˆëŠ” í† í°
    ì´ëŸ° ê²½ìš°ì—ëŠ” ì–µì§€ fuzzy ë§¤í•‘í•˜ì§€ ì•Šê³  ê·¸ëŒ€ë¡œ ë‚¨ê²¨ë‘ê³ ,
    í•„ìš”í•˜ë©´ ë‚˜ì¤‘ì— 'í…ìŠ¤íŠ¸ë¡œë§Œ ì†¡ì¶œ' í›„ë³´ë¡œ ì‚¬ìš©.
    """
    t = norm(tok)

    # ì˜ë¬¸ìë‚˜ ìˆ«ì í¬í•¨
    if re.search(r"[A-Za-z0-9]", t):
        return True

    # ì€í–‰ ê´€ë ¨ í‚¤ì›Œë“œ í¬í•¨ (í•„ìš”í•˜ë©´ ë” ì¶”ê°€ ê°€ëŠ¥)
    bank_keywords = ["KB", "ì¼€ì´ë¹„", "êµ­ë¯¼", "ì‹ í•œ", "ìš°ë¦¬", "í•˜ë‚˜"]
    if any(kw in t for kw in bank_keywords):
        return True

    # ê³µë°± ì—†ì´ ê½¤ ê¸´ í† í° (ì˜ˆ: ê³¨ë“œë¼ì´í”„ìŠ¤ë§ˆíŠ¸í”ŒëŸ¬ìŠ¤)
    if " " not in t and len(t) >= 8:
        return True

    return False


# -------------------------------
# ğŸ” ìœ ì‚¬ë„/ë¶€ë¶„ì¼ì¹˜ í›„ë³´ ì°¾ëŠ” í—¬í¼ë“¤
# -------------------------------

def suggest_by_substring(tok: str, dict_terms: set[str]) -> list[str]:
    """tok ì•ˆì— í¬í•¨ë˜ê±°ë‚˜ tokë¥¼ í¬í•¨í•˜ëŠ” ì‚¬ì „ ë‹¨ì–´ë“¤ (í•œ ê¸€ìëŠ” ì œì™¸)."""
    t = norm(tok)
    out = []
    for term in dict_terms:
        if not term:
            continue

        # í•œ ê¸€ìì§œë¦¬ ë‹¨ì–´(ex: 'ê³ ','ê¸ˆ','ë¦¬') ì œì™¸
        if len(term) < 2:
            continue

        # í¬í•¨ ì—¬ë¶€ ì²´í¬
        if term in t or t in term:
            out.append(term)

    return sorted(set(out))


def suggest_by_split(tok: str, dict_terms: set[str]) -> list[list[str]]:
    """
    tokë¥¼ ì•/ë’¤ë¡œ ìª¼ê°œì„œ ë‘˜ ë‹¤ ì‚¬ì „ì— ìˆìœ¼ë©´ ì¡°í•©ìœ¼ë¡œ ì œì•ˆ.
    ì˜ˆ: 'ê³ ì •ê¸ˆë¦¬' -> ['ê³ ì •', 'ê¸ˆë¦¬']
    """
    t = norm(tok)
    candidates: list[list[str]] = []

    if len(t) < 2:
        return candidates

    for i in range(1, len(t)):
        left, right = t[:i], t[i:]
        if left in dict_terms and right in dict_terms:
            candidates.append([left, right])

    return candidates


def suggest_by_fuzzy(tok: str, dict_terms: set[str], n: int = 5, cutoff: float = 0.7) -> list[str]:
    """
    difflib ê¸°ë°˜ fuzzy ë§¤ì¹­.
    - í•œ ê¸€ìì§œë¦¬ ì‚¬ì „ ë‹¨ì–´ëŠ” í›„ë³´ í’€ì—ì„œ ì œì™¸
    - cutoffë¥¼ ì¡°ê¸ˆ ì˜¬ë ¤ì„œ ë…¸ì´ì¦ˆ ì¤„ì„
    """
    t = norm(tok)

    # í•œ ê¸€ì ë‹¨ì–´ ì œì™¸í•œ í’€ë§Œ ì‚¬ìš©
    pool = [term for term in dict_terms if len(term) >= 2]

    if not pool:
        return []

    return get_close_matches(t, pool, n=n, cutoff=cutoff)


def build_missing_map(missing_tokens: set[str], dict_terms: set[str]) -> dict:
    """
    ì‚¬ì „ì— ì—†ëŠ” í† í°ë“¤ì— ëŒ€í•´ substring / split / fuzzy í›„ë³´ë¥¼ ê°™ì´ ì €ì¥.
    - ë¸Œëœë“œ/ìƒí’ˆëª… ìŠ¤íƒ€ì¼ í† í°ì€ fuzzy ë§¤ì¹­ì„ ìƒëµ (ê´œíˆ 'ì˜ˆê¸ˆ','ìƒí’ˆ' ê°™ì€ ê±¸ë¡œ ì¼ë°˜í™”í•˜ì§€ ì•Šê¸° ìœ„í•¨)
    """
    result = {}

    for tok in sorted(missing_tokens):
        substr = suggest_by_substring(tok, dict_terms)
        split  = suggest_by_split(tok, dict_terms)

        # ë¸Œëœë“œ/ìƒí’ˆëª… ìŠ¤íƒ€ì¼ì´ë©´ fuzzyëŠ” ì•„ì˜ˆ ëŒë¦¬ì§€ ì•ŠìŒ
        if is_brand_like(tok):
            fuzzy = []
        else:
            fuzzy = suggest_by_fuzzy(tok, dict_terms)

        result[tok] = {
            "substring": substr,
            "split": split,
            "fuzzy": fuzzy,
        }

    return result


def main():
    if not DICT_CSV_PATH.exists():
        raise FileNotFoundError(f"ì‚¬ì „ CSVë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {DICT_CSV_PATH}")
    if not GLOSS_TOKENS.exists():
        raise FileNotFoundError(f"gloss_tokens.txtë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {GLOSS_TOKENS}")

    # 1) ì‚¬ì „ ë‹¨ì–´ ë¡œë“œ
    dict_terms = load_korean_terms_from_dict(DICT_CSV_PATH)
    print(f"[Info] ì‚¬ì „ ë‹¨ì–´ ê°œìˆ˜: {len(dict_terms)}")

    # 2) gloss_tokens.txtì—ì„œ ì‚¬ì „ì— ì—†ëŠ” ë‹¨ì–´ ìˆ˜ì§‘
    missing_set = set()
    total_tokens = 0

    with open(GLOSS_TOKENS, "r", encoding="utf-8") as f:
        for line in f:
            tok = line.strip()
            if not tok:
                continue
            total_tokens += 1
            if norm(tok) not in dict_terms:
                missing_set.add(tok)

    # 3) ìœ ì‚¬ë„/ë¶€ë¶„ì¼ì¹˜ ì •ë³´ë¥¼ í¬í•¨í•œ missing map ìƒì„±
    missing_map = build_missing_map(missing_set, dict_terms)

    # 4) JSON íŒŒì¼ë¡œ ì €ì¥
    with open(MISSING_JSON, "w", encoding="utf-8") as f:
        json.dump(missing_map, f, ensure_ascii=False, indent=2)

    print(f"[Done] gloss_tokens ë‹¨ì–´ ìˆ˜: {total_tokens}")
    print(f"[Done] ì‚¬ì „ì— ì—†ëŠ” ë‹¨ì–´ ìˆ˜: {len(missing_set)}")
    print(f"[Done] JSON ì €ì¥: {MISSING_JSON}")


if __name__ == "__main__":
    main()
