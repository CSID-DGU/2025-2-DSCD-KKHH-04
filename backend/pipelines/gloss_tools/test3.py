# -*- coding: utf-8 -*-
"""
1) gloss_dictionary_MOCK_1.csvì˜ korean_meanings ì—´ì—ì„œ ëª¨ë“  í•œêµ­ì–´ ë‹¨ì–´ ëª©ë¡ ìˆ˜ì§‘
2) gloss_tokens.txtì˜ ë‹¨ì–´ê°€ ì‚¬ì „ì— ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
3) ì‚¬ì „ì— ì—†ëŠ” ë‹¨ì–´ë“¤ì— ëŒ€í•´ substring/split/fuzzy ê¸°ë°˜ í›„ë³´ë¥¼ ìƒì„±í•˜ì—¬ JSON ì €ì¥
"""

import csv
import ast
import unicodedata
import re
import json
from pathlib import Path
from difflib import get_close_matches  # fuzzy ê²€ìƒ‰

ROOT_DIR = Path(__file__).resolve().parent

DICT_CSV_PATH   = ROOT_DIR / "gloss_dictionary_MOCK_1.csv"
GLOSS_TOKENS    = ROOT_DIR / "gloss_tokens.txt"
MISSING_JSON    = ROOT_DIR / "gloss_missing_map3.json"


def norm(s: str) -> str:
    """ì „ê°/ë°˜ê° í†µì¼ + ê³µë°± ì •ê·œí™”."""
    s = unicodedata.normalize("NFKC", s or "").strip()
    return re.sub(r"\s+", " ", s)


def load_korean_terms_from_dict(csv_path: Path) -> set[str]:
    """CSV ì‚¬ì „ì—ì„œ korean_meanings ê³„ì—´ ì»¬ëŸ¼ì˜ í•œêµ­ì–´ ë‹¨ì–´ë“¤ì„ setìœ¼ë¡œ ë¡œë“œ."""
    terms = set()

    with open(csv_path, "r", encoding="utf-8-sig") as f:
        rdr = csv.DictReader(f)
        headers = [h.lower().strip() for h in (rdr.fieldnames or [])]

        cand = [
            "korean_meanings", "korean", "ko",
            "meaning_ko", "ko_meanings", "korean_meaning",
        ]
        h_ko = next((c for c in cand if c in headers), None)

        if not h_ko:
            raise RuntimeError(f"korean_meanings ì—´ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í—¤ë”: {headers}")

        for row in rdr:
            cell = (row.get(h_ko) or "").strip()
            if not cell:
                continue

            try:
                obj = ast.literal_eval(cell)
                if isinstance(obj, (list, tuple)):
                    items = [str(x) for x in obj]
                else:
                    items = [str(obj)]
            except Exception:
                items = [cell]

            for t in items:
                t_norm = norm(t)
                if t_norm:
                    terms.add(t_norm)

    return terms


# -------------------------------
# ğŸ” ìœ ì‚¬ë„/ë¶€ë¶„ì¼ì¹˜ í›„ë³´ ì°¾ëŠ” í—¬í¼ë“¤
# -------------------------------

def suggest_by_substring(tok: str, dict_terms: set[str]) -> list[str]:
    """tok ì•ˆì— í¬í•¨ë˜ê±°ë‚˜ tokë¥¼ í¬í•¨í•˜ëŠ” ì‚¬ì „ ë‹¨ì–´ë“¤ (í•œ ê¸€ìëŠ” ì œì™¸)."""
    t = norm(tok)
    out = []
    for term in dict_terms:
        if not term:
            continue

        # ğŸ”¥ í•œ ê¸€ìì§œë¦¬ ë‹¨ì–´(ex: 'ê³ ','ê¸ˆ','ë¦¬') ì œì™¸
        if len(term) < 2:
            continue

        # í¬í•¨ ì—¬ë¶€ ì²´í¬
        if term in t or t in term:
            out.append(term)

    return sorted(set(out))


def suggest_by_split(tok: str, dict_terms: set[str]) -> list[list[str]]:
    """
    tokë¥¼ ì•/ë’¤ë¡œ ìª¼ê°œì„œ ë‘˜ ë‹¤ ì‚¬ì „ì— ìˆìœ¼ë©´ ì¡°í•©ìœ¼ë¡œ ì œì•ˆ.
    ì˜ˆ: 'ê³ ì •ê¸ˆë¦¬' -> ['ê³ ì •', 'ê¸ˆë¦¬']
    """
    t = norm(tok)
    candidates: list[list[str]] = []

    if len(t) < 2:
        return candidates

    for i in range(1, len(t)):
        left, right = t[:i], t[i:]
        if left in dict_terms and right in dict_terms:
            candidates.append([left, right])

    return candidates


def suggest_by_fuzzy(tok: str, dict_terms: set[str], n: int = 5, cutoff: float = 0.6) -> list[str]:
    """difflib ê¸°ë°˜ fuzzy ë§¤ì¹­."""
    t = norm(tok)
    return get_close_matches(t, list(dict_terms), n=n, cutoff=cutoff)


def build_missing_map(missing_tokens: set[str], dict_terms: set[str]) -> dict:
    """
    ì‚¬ì „ì— ì—†ëŠ” í† í°ë“¤ì— ëŒ€í•´ substring / split / fuzzy í›„ë³´ë¥¼ ê°™ì´ ì €ì¥.
    """
    result = {}

    for tok in sorted(missing_tokens):
        substr = suggest_by_substring(tok, dict_terms)
        split  = suggest_by_split(tok, dict_terms)
        fuzzy  = suggest_by_fuzzy(tok, dict_terms)

        result[tok] = {
            "substring": substr,
            "split": split,
            "fuzzy": fuzzy,
        }

    return result


def main():
    if not DICT_CSV_PATH.exists():
        raise FileNotFoundError(f"ì‚¬ì „ CSVë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {DICT_CSV_PATH}")
    if not GLOSS_TOKENS.exists():
        raise FileNotFoundError(f"gloss_tokens.txtë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {GLOSS_TOKENS}")

    # 1) ì‚¬ì „ ë‹¨ì–´ ë¡œë“œ
    dict_terms = load_korean_terms_from_dict(DICT_CSV_PATH)
    print(f"[Info] ì‚¬ì „ ë‹¨ì–´ ê°œìˆ˜: {len(dict_terms)}")

    # 2) gloss_tokens.txtì—ì„œ ì‚¬ì „ì— ì—†ëŠ” ë‹¨ì–´ ìˆ˜ì§‘
    missing_set = set()
    total_tokens = 0

    with open(GLOSS_TOKENS, "r", encoding="utf-8") as f:
        for line in f:
            tok = line.strip()
            if not tok:
                continue
            total_tokens += 1
            if norm(tok) not in dict_terms:
                missing_set.add(tok)

    # 3) ìœ ì‚¬ë„/ë¶€ë¶„ì¼ì¹˜ ì •ë³´ë¥¼ í¬í•¨í•œ missing map ìƒì„±
    missing_map = build_missing_map(missing_set, dict_terms)

    # 4) JSON íŒŒì¼ë¡œ ì €ì¥
    with open(MISSING_JSON, "w", encoding="utf-8") as f:
        json.dump(missing_map, f, ensure_ascii=False, indent=2)

    print(f"[Done] gloss_tokens ë‹¨ì–´ ìˆ˜: {total_tokens}")
    print(f"[Done] ì‚¬ì „ì— ì—†ëŠ” ë‹¨ì–´ ìˆ˜: {len(missing_set)}")
    print(f"[Done] JSON ì €ì¥: {MISSING_JSON}")


if __name__ == "__main__":
    main()
