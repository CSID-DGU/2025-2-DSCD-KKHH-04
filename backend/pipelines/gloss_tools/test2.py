import csv
import ast
import unicodedata
import re
import json
from pathlib import Path
from difflib import get_close_matches  # âœ… ì¶”ê°€

ROOT_DIR = Path(__file__).resolve().parent

DICT_CSV_PATH   = ROOT_DIR / "gloss_dictionary_MOCK_1.csv"
GLOSS_TOKENS    = ROOT_DIR / "gloss_tokens.txt"
MISSING_JSON    = ROOT_DIR / "gloss_missing_map2.json"


def norm(s: str) -> str:
    """ì „ê°/ë°˜ê° í†µì¼ + ê³µë°± ì •ê·œí™”."""
    s = unicodedata.normalize("NFKC", s or "").strip()
    return re.sub(r"\s+", " ", s)


def load_korean_terms_from_dict(csv_path: Path) -> set[str]:
    """CSV ì‚¬ì „ì—ì„œ korean_meanings ê³„ì—´ ì»¬ëŸ¼ì˜ í•œêµ­ì–´ ë‹¨ì–´ë“¤ì„ setìœ¼ë¡œ ë¡œë“œ."""
    terms = set()

    with open(csv_path, "r", encoding="utf-8-sig") as f:
        rdr = csv.DictReader(f)
        headers = [h.lower().strip() for h in (rdr.fieldnames or [])]

        cand = [
            "korean_meanings", "korean", "ko",
            "meaning_ko", "ko_meanings", "korean_meaning",
        ]
        h_ko = next((c for c in cand if c in headers), None)

        if not h_ko:
            raise RuntimeError(f"korean_meanings ì—´ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í—¤ë”: {headers}")

        for row in rdr:
            cell = (row.get(h_ko) or "").strip()
            if not cell:
                continue

            # '["ê±°ì¹˜","ê±°ì¹˜ì‹"]' ê°™ì€ ë¦¬ìŠ¤íŠ¸ ë¬¸ìì—´ ì²˜ë¦¬
            try:
                obj = ast.literal_eval(cell)
                if isinstance(obj, (list, tuple)):
                    items = [str(x) for x in obj]
                else:
                    items = [str(obj)]
            except Exception:
                items = [cell]

            for t in items:
                t_norm = norm(t)
                if t_norm:
                    terms.add(t_norm)

    return terms


# -------------------------------
# ğŸ” ìœ ì‚¬ë„/ë¶€ë¶„ì¼ì¹˜ í›„ë³´ ì°¾ëŠ” í—¬í¼ë“¤
# -------------------------------

def suggest_by_substring(tok: str, dict_terms: set[str]) -> list[str]:
    """tok ì•ˆì— í¬í•¨ë˜ê±°ë‚˜ tokë¥¼ í¬í•¨í•˜ëŠ” ì‚¬ì „ ë‹¨ì–´ë“¤."""
    t = norm(tok)
    out = []
    for term in dict_terms:
        if not term:
            continue
        if term in t or t in term:
            out.append(term)
    # ê³ ì •ê¸ˆë¦¬ -> ê³ ì •, ê¸ˆë¦¬ ë‘˜ ë‹¤ ì¡í˜€ë„ ìˆœì„œëŠ” í¬ê²Œ ìƒê´€ì—†ìœ¼ë‹ˆ ì •ë ¬
    return sorted(set(out))


def suggest_by_split(tok: str, dict_terms: set[str]) -> list[list[str]]:
    """
    tokë¥¼ ì•/ë’¤ë¡œ ìª¼ê°œì„œ ë‘˜ ë‹¤ ì‚¬ì „ì— ìˆìœ¼ë©´ ì¡°í•©ìœ¼ë¡œ ì œì•ˆ.
    ì˜ˆ: 'ê³ ì •ê¸ˆë¦¬' -> ['ê³ ì •', 'ê¸ˆë¦¬']
    """
    t = norm(tok)
    candidates: list[list[str]] = []

    # ë„ˆë¬´ ì§§ì€ ê±´ ìª¼ê°¤ í•„ìš” ì—†ìŒ
    if len(t) < 2:
        return candidates

    for i in range(1, len(t)):
        left, right = t[:i], t[i:]
        if left in dict_terms and right in dict_terms:
            candidates.append([left, right])

    return candidates


def suggest_by_fuzzy(tok: str, dict_terms: set[str], n: int = 5, cutoff: float = 0.6) -> list[str]:
    """
    difflib ê¸°ë°˜ fuzzy ë§¤ì¹­ (ì² ì ë¹„ìŠ·í•œ ì• ë“¤).
    """
    t = norm(tok)
    # get_close_matchesëŠ” ë¦¬ìŠ¤íŠ¸ë¥¼ ìš”êµ¬
    matches = get_close_matches(t, list(dict_terms), n=n, cutoff=cutoff)
    return matches


def build_missing_map(missing_tokens: set[str], dict_terms: set[str]) -> dict:
    result = {}

    for tok in sorted(missing_tokens):
        substr = suggest_by_substring(tok, dict_terms)
        split  = suggest_by_split(tok, dict_terms)  # [['ê³ ì •','ê¸ˆë¦¬']]
        fuzzy  = suggest_by_fuzzy(tok, dict_terms)

        merged = []

        # substring ë¨¼ì €
        for x in substr:
            if x not in merged:
                merged.append(x)

        # split ê²°ê³¼ ì¶”ê°€
        for pair in split:         # [['ê³ ì •','ê¸ˆë¦¬']]
            for x in pair:
                if x not in merged:
                    merged.append(x)

        # fuzzy ì¶”ê°€
        for x in fuzzy:
            if x not in merged:
                merged.append(x)

        result[tok] = merged

    return result


def main():
    if not DICT_CSV_PATH.exists():
        raise FileNotFoundError(f"ì‚¬ì „ CSVë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {DICT_CSV_PATH}")
    if not GLOSS_TOKENS.exists():
        raise FileNotFoundError(f"gloss_tokens.txtë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {GLOSS_TOKENS}")

    # 1) ì‚¬ì „ ë‹¨ì–´ ë¡œë“œ
    dict_terms = load_korean_terms_from_dict(DICT_CSV_PATH)
    print(f"[Info] ì‚¬ì „ ë‹¨ì–´ ê°œìˆ˜: {len(dict_terms)}")

    # 2) gloss_tokens.txtì—ì„œ ì‚¬ì „ì— ì—†ëŠ” ë‹¨ì–´ ìˆ˜ì§‘
    missing_set = set()
    total_tokens = 0

    with open(GLOSS_TOKENS, "r", encoding="utf-8") as f:
        for line in f:
            tok = line.strip()
            if not tok:
                continue
            total_tokens += 1
            if norm(tok) not in dict_terms:
                missing_set.add(tok)

    # 3) ìœ ì‚¬ë„/ë¶€ë¶„ì¼ì¹˜ ì •ë³´ë¥¼ í¬í•¨í•œ missing map ìƒì„±
    missing_map = build_missing_map(missing_set, dict_terms)

    # 4) JSON íŒŒì¼ë¡œ ì €ì¥
    with open(MISSING_JSON, "w", encoding="utf-8") as f:
        json.dump(missing_map, f, ensure_ascii=False, indent=2)

    print(f"[Done] gloss_tokens ë‹¨ì–´ ìˆ˜: {total_tokens}")
    print(f"[Done] ì‚¬ì „ì— ì—†ëŠ” ë‹¨ì–´ ìˆ˜: {len(missing_set)}")
    print(f"[Done] JSON ì €ì¥: {MISSING_JSON}")


if __name__ == "__main__":
    main()




