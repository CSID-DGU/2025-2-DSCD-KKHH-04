{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0ce36db",
   "metadata": {},
   "source": [
    "### ê·œì¹™ ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3999c38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. ë°ì´í„° ë¡œë“œ ì¤‘...\n",
      "   - DB ìœ íš¨ í‚¤ ê°œìˆ˜: 7741\n",
      "2. ë¶„ì„ ëŒ€ìƒ ë¯¸ë“±ì¬ ë‹¨ì–´: 0ê°œ (ìˆ«ì ë“± ì œì™¸ë¨: 277)\n",
      "   - ì²˜ë¦¬í•  ë‹¨ì–´ê°€ ì—†ìŠµë‹ˆë‹¤. ì¢…ë£Œí•©ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import json\n",
    "import csv\n",
    "import re\n",
    "import random\n",
    "import ast\n",
    "import time\n",
    "from pathlib import Path\n",
    "import google.generativeai as genai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# --- ì„¤ì • ---\n",
    "load_dotenv()\n",
    "API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "MODEL_NAME = \"models/gemini-2.5-flash\"\n",
    "\n",
    "\n",
    "# Jupyterì—ì„œëŠ” __file__ ì‚¬ìš© ë¶ˆê°€ â†’ í˜„ì¬ ì‘ì—… ê²½ë¡œë¡œ ì²˜ë¦¬\n",
    "ROOT_DIR = Path(os.getcwd())\n",
    "\n",
    "# gloss í´ë”\n",
    "GLOSS_DIR = ROOT_DIR / \"gloss\"\n",
    "\n",
    "\n",
    "# íŒŒì¼ë“¤ (ëª¨ë‘ gloss ì•ˆì— ìˆìŒ)\n",
    "CSV_PATH        = GLOSS_DIR / \"gloss_dictionary_MOCK.csv\"\n",
    "RULES_JSON_PATH = GLOSS_DIR / \"rules.json\"\n",
    "TOKENS_PATH     = GLOSS_DIR / \"gloss_tokens.txt\"\n",
    "SCRIPT_PATH     = GLOSS_DIR / \"script.txt\"\n",
    "\n",
    "\n",
    "def norm(s):\n",
    "    return str(s).strip().replace(\" \", \"\")\n",
    "\n",
    "# 1. DBì— ìˆëŠ” ëª¨ë“  ìœ íš¨ ë‹¨ì–´(Key) ë¡œë“œ\n",
    "def load_db_keys(csv_path):\n",
    "    valid_keys = set()\n",
    "    if not csv_path.exists():\n",
    "        print(\"[Error] CSV íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        return valid_keys\n",
    "    \n",
    "    with open(csv_path, 'r', encoding='utf-8') as f:\n",
    "        # csv ëª¨ë“ˆ ëŒ€ì‹  ì§ì ‘ íŒŒì‹± (korean_meanings êµ¬ì¡° ë•Œë¬¸)\n",
    "        # pandas ì—†ì´ ê°€ë³ê²Œ ì²˜ë¦¬\n",
    "        lines = f.readlines()\n",
    "        for line in lines[1:]: # í—¤ë” ê±´ë„ˆëœ€\n",
    "            # ë‹¨ìˆœ íŒŒì‹±: ëŒ€ê´„í˜¸ ì•ˆì˜ ë‚´ìš© ì¶”ì¶œ\n",
    "            match = re.search(r\"\\[(.*?)\\]\", line)\n",
    "            if match:\n",
    "                content = match.group(1)\n",
    "                # 'íŒŒë‘', 'ì²­' -> íŒŒì‹±\n",
    "                words = [w.strip().replace(\"'\", \"\").replace('\"', \"\") for w in content.split(',')]\n",
    "                for w in words:\n",
    "                    if w: valid_keys.add(w)\n",
    "    return valid_keys\n",
    "\n",
    "# 2. í† í°ì˜ ë¬¸ë§¥(ì˜ˆì‹œ ë¬¸ì¥) ì°¾ê¸°\n",
    "def get_context_sentence(token, script_lines):\n",
    "    # í•´ë‹¹ í† í°ì´ í¬í•¨ëœ ë¬¸ì¥ ì¤‘ í•˜ë‚˜ë¥¼ ëœë¤ìœ¼ë¡œ ë°˜í™˜ (ë„ˆë¬´ ê¸¸ë©´ ì•ë¶€ë¶„ë§Œ)\n",
    "    candidates = [line.strip() for line in script_lines if token in line]\n",
    "    if candidates:\n",
    "        return random.choice(candidates)\n",
    "    return \"ë¬¸ë§¥ ì—†ìŒ\"\n",
    "\n",
    "# 3. Geminiì—ê²Œ ë§¤í•‘ ìš”ì²­\n",
    "def ask_gemini_mapping(unknown_words_with_context):\n",
    "    # unknown_words_with_context = [{\"word\": \"...\", \"context\": \"...\"}, ...]\n",
    "    if not unknown_words_with_context:\n",
    "        return {}\n",
    "\n",
    "    genai.configure(api_key=API_KEY)\n",
    "    model = genai.GenerativeModel(MODEL_NAME)\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    ë‹¹ì‹ ì€ ìˆ˜ì–´ ë²ˆì—­ ë°ì´í„° ì „ì²˜ë¦¬ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.\n",
    "    ì•„ë˜ ì£¼ì–´ì§„ 'ë‹¨ì–´(word)'ë“¤ì€ í˜„ì¬ ìˆ˜ì–´ ë°ì´í„°ë² ì´ìŠ¤ì— ì—†ëŠ” ë‹¨ì–´ë“¤ì…ë‹ˆë‹¤.\n",
    "    í•¨ê»˜ ì œê³µëœ 'ë¬¸ë§¥(context)'ì„ ë³´ê³ , ì´ ë‹¨ì–´ë¥¼ **ì˜ë¯¸ê°€ ê°€ì¥ ìœ ì‚¬í•œ 'ê¸°ì´ˆ ë‹¨ì–´'** ë˜ëŠ” **'ê¸°ì´ˆ ë‹¨ì–´ë“¤ì˜ ì¡°í•©'**ìœ¼ë¡œ ë³€í™˜í•´ì£¼ì„¸ìš”.\n",
    "\n",
    "    [ê·œì¹™]\n",
    "    1. **Synonym(ë™ì˜ì–´):** 1:1ë¡œ ëŒ€ì²´ ê°€ëŠ¥í•œ ê¸°ì´ˆ ëª…ì‚¬/ë™ì‚¬ (ì˜ˆ: \"ìˆ˜ë ¹\" -> [\"ë°›ë‹¤\"])\n",
    "    2. **Compound(ë³µí•©ì–´):** ì˜ë¯¸ ë‹¨ìœ„ë¡œ ë¶„í•´ (ì˜ˆ: \"ìš°ëŒ€ê¸ˆë¦¬\" -> [\"íŠ¹ë³„\", \"ê¸ˆë¦¬\"])\n",
    "    3. **Uncertain(ë¶ˆí™•ì‹¤):** ë„ì €íˆ ê¸°ì´ˆ ë‹¨ì–´ë¡œ í‘œí˜„ì´ ì•ˆ ë˜ê±°ë‚˜, ì‚¬ëŒì˜ í™•ì¸ì´ ê¼­ í•„ìš”í•œ ê²½ìš° (ì˜ˆ: ê³ ìœ ëª…ì‚¬, ì‹ ì¡°ì–´) -> \"targets\": [] ë¡œ ë¹„ì›Œë‘˜ ê²ƒ.\n",
    "    \n",
    "    [ì…ë ¥ ë°ì´í„°]\n",
    "    {json.dumps(unknown_words_with_context, ensure_ascii=False)}\n",
    "\n",
    "    [ì¶œë ¥ í˜•ì‹ - JSON Only]\n",
    "    {{\n",
    "      \"results\": [\n",
    "        {{ \"word\": \"ìˆ˜ë ¹\", \"targets\": [\"ë°›ë‹¤\"], \"type\": \"synonym\" }},\n",
    "        {{ \"word\": \"ìš°ëŒ€ê¸ˆë¦¬\", \"targets\": [\"íŠ¹ë³„\", \"ê¸ˆë¦¬\"], \"type\": \"compound\" }},\n",
    "        {{ \"word\": \"ì´ìƒí•œë‹¨ì–´\", \"targets\": [], \"type\": \"uncertain\" }}\n",
    "      ]\n",
    "    }}\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        resp = model.generate_content(prompt, generation_config={\"response_mime_type\": \"application/json\"})\n",
    "        return json.loads(resp.text).get(\"results\", [])\n",
    "    except Exception as e:\n",
    "        print(f\"[Error] Gemini API í˜¸ì¶œ ì‹¤íŒ¨: {e}\")\n",
    "        return []\n",
    "\n",
    "def main():\n",
    "    # --- ë°ì´í„° ë¡œë“œ ---\n",
    "    print(\"1. ë°ì´í„° ë¡œë“œ ì¤‘...\")\n",
    "    db_keys = load_db_keys(CSV_PATH)\n",
    "    print(f\"   - DB ìœ íš¨ í‚¤ ê°œìˆ˜: {len(db_keys)}\")\n",
    "\n",
    "    # rules.json ë¡œë“œ (ì—†ìœ¼ë©´ ìƒì„±)\n",
    "    if RULES_JSON_PATH.exists():\n",
    "        with open(RULES_JSON_PATH, 'r', encoding='utf-8') as f:\n",
    "            rules = json.load(f)\n",
    "    else:\n",
    "        rules = {\"disambiguation_rules\": {}, \"fixed_mappings\": {}, \"word_substitution\": {}, \"manual_review_required\": {}}\n",
    "    \n",
    "    # ëˆ„ë½ëœ í•„ë“œ ì´ˆê¸°í™”\n",
    "    if \"word_substitution\" not in rules: rules[\"word_substitution\"] = {}\n",
    "    if \"manual_review_required\" not in rules: rules[\"manual_review_required\"] = {}\n",
    "\n",
    "    # ëŒ€ë³¸ ë¡œë“œ\n",
    "    with open(SCRIPT_PATH, 'r', encoding='utf-8') as f:\n",
    "        script_lines = f.readlines()\n",
    "\n",
    "    # í† í° ë¡œë“œ\n",
    "    with open(TOKENS_PATH, 'r', encoding='utf-8') as f:\n",
    "        tokens = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "    # --- ì²˜ë¦¬ ëŒ€ìƒ í•„í„°ë§ ---\n",
    "    target_tokens = []\n",
    "    skipped_count = 0\n",
    "    \n",
    "    # ì •ê·œì‹: ìˆ«ì í¬í•¨, í˜¹ì€ 1ê¸€ì íŠ¹ìˆ˜ë¬¸ì ë“± ì œì™¸\n",
    "    regex_skip = re.compile(r\"[0-9]|[.,!?;:]\")\n",
    "\n",
    "    for t in tokens:\n",
    "        # 1) ì´ë¯¸ DBì— ìˆìœ¼ë©´ íŒ¨ìŠ¤\n",
    "        if t in db_keys:\n",
    "            continue\n",
    "        # 2) ì´ë¯¸ ê·œì¹™ì— ìˆìœ¼ë©´ íŒ¨ìŠ¤\n",
    "        if t in rules[\"word_substitution\"] or t in rules[\"manual_review_required\"]:\n",
    "            continue\n",
    "        # 3) ìˆ«ì/íŠ¹ìˆ˜ë¬¸ì í¬í•¨ì´ë©´ íŒ¨ìŠ¤ (ì´ë¯¸ì§€ ì²˜ë¦¬ ì˜ì—­)\n",
    "        if regex_skip.search(t):\n",
    "            skipped_count += 1\n",
    "            continue\n",
    "            \n",
    "        target_tokens.append(t)\n",
    "\n",
    "    # ì¤‘ë³µ ì œê±°\n",
    "    target_tokens = sorted(list(set(target_tokens)))\n",
    "    print(f\"2. ë¶„ì„ ëŒ€ìƒ ë¯¸ë“±ì¬ ë‹¨ì–´: {len(target_tokens)}ê°œ (ìˆ«ì ë“± ì œì™¸ë¨: {skipped_count})\")\n",
    "\n",
    "    if not target_tokens:\n",
    "        print(\"   - ì²˜ë¦¬í•  ë‹¨ì–´ê°€ ì—†ìŠµë‹ˆë‹¤. ì¢…ë£Œí•©ë‹ˆë‹¤.\")\n",
    "        return\n",
    "\n",
    "    # --- ë°°ì¹˜ ì²˜ë¦¬ (LLM) ---\n",
    "    BATCH_SIZE = 20\n",
    "    processed_count = 0\n",
    "    auto_success = 0\n",
    "    manual_flagged = 0\n",
    "\n",
    "    print(\"3. LLM ë¶„ì„ ë° ê·œì¹™ ìƒì„± ì‹œì‘...\")\n",
    "    \n",
    "    for i in range(0, len(target_tokens), BATCH_SIZE):\n",
    "        batch = target_tokens[i:i+BATCH_SIZE]\n",
    "        batch_input = []\n",
    "        \n",
    "        # ë¬¸ë§¥ ë§¤í•‘\n",
    "        for token in batch:\n",
    "            ctx = get_context_sentence(token, script_lines)\n",
    "            batch_input.append({\"word\": token, \"context\": ctx})\n",
    "        \n",
    "        # LLM í˜¸ì¶œ\n",
    "        print(f\"   - Processing batch {i+1}/{len(target_tokens)}...\")\n",
    "        results = ask_gemini_mapping(batch_input)\n",
    "        \n",
    "        # ê²°ê³¼ ê²€ì¦ ë° ë¶„ë¥˜\n",
    "        for res in results:\n",
    "            word = res.get(\"word\")\n",
    "            targets = res.get(\"targets\")\n",
    "            \n",
    "            if not targets: # LLMì´ í¬ê¸°í•¨ (Uncertain)\n",
    "                rules[\"manual_review_required\"][word] = {\n",
    "                    \"reason\": \"LLM Uncertain\",\n",
    "                    \"context\": get_context_sentence(word, script_lines)\n",
    "                }\n",
    "                manual_flagged += 1\n",
    "                continue\n",
    "\n",
    "            # ê²€ì¦: Targetsê°€ ì‹¤ì œ DBì— ìˆëŠ”ê°€?\n",
    "            all_valid = True\n",
    "            invalid_parts = []\n",
    "            for t_part in targets:\n",
    "                if t_part not in db_keys:\n",
    "                    all_valid = False\n",
    "                    invalid_parts.append(t_part)\n",
    "            \n",
    "            if all_valid:\n",
    "                # ì„±ê³µ -> word_substitutionì— ì¶”ê°€\n",
    "                rules[\"word_substitution\"][word] = targets\n",
    "                auto_success += 1\n",
    "            else:\n",
    "                # ì‹¤íŒ¨ -> ì‚¬ëŒì´ ë´ì•¼ í•¨ (LLMì´ ì—†ëŠ” ë‹¨ì–´ë¥¼ ì¶”ì²œí•¨)\n",
    "                rules[\"manual_review_required\"][word] = {\n",
    "                    \"reason\": f\"Target not in DB: {invalid_parts}\",\n",
    "                    \"suggested\": targets,\n",
    "                    \"context\": get_context_sentence(word, script_lines)\n",
    "                }\n",
    "                manual_flagged += 1\n",
    "                \n",
    "        # API Rate Limit ê³ ë ¤ (í•„ìš”ì‹œ)\n",
    "        time.sleep(1) \n",
    "\n",
    "    # --- ê²°ê³¼ ì €ì¥ ---\n",
    "    with open(RULES_JSON_PATH, 'w', encoding='utf-8') as f:\n",
    "        json.dump(rules, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(f\"[ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½]\")\n",
    "    print(f\"1. ì´ ë¶„ì„ ëŒ€ìƒ : {len(target_tokens)} ê°œ\")\n",
    "    print(f\"2. ìë™ í•´ê²°    : {auto_success} ê°œ (word_substitutionì— ì¶”ê°€ë¨)\")\n",
    "    print(f\"3. ìˆ˜ì‘ì—… í•„ìš”  : {manual_flagged} ê°œ (manual_review_requiredì— ì¶”ê°€ë¨)\")\n",
    "    print(f\"   -> ê²½ë¡œ: {RULES_JSON_PATH}\")\n",
    "    print(\"=\"*40)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf35b35",
   "metadata": {},
   "source": [
    "### ì‹œê°í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c053566f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âŒ íŒŒì¼ì´ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "# [Cell 2] ì™¸ë¶€ í† í° ë§¤í•‘ í˜„í™© ì‹œê°í™”\n",
    "import json\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# íŒŒì¼ ê²½ë¡œ ì„¤ì •\n",
    "CSV_PATH = \"gloss_dictionary_MOCK.csv\"\n",
    "RULES_PATH = \"rules.json\"\n",
    "TOKENS_PATH = \"gloss_tokens.txt\"\n",
    "\n",
    "def visualize_status():\n",
    "    if not os.path.exists(RULES_PATH) or not os.path.exists(TOKENS_PATH):\n",
    "        print(\"âŒ íŒŒì¼ì´ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
    "        return\n",
    "\n",
    "    # 1. ë°ì´í„° ë¡œë“œ\n",
    "    db_keys = set()\n",
    "    with open(CSV_PATH, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "        for m in re.findall(r\"\\[(.*?)\\]\", content):\n",
    "            for w in m.replace(\"'\", \"\").replace('\"', \"\").split(','):\n",
    "                if w.strip(): db_keys.add(w.strip())\n",
    "\n",
    "    with open(RULES_PATH, 'r', encoding='utf-8') as f:\n",
    "        rules = json.load(f)\n",
    "    with open(TOKENS_PATH, 'r', encoding='utf-8') as f:\n",
    "        tokens = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "    # 2. í†µê³„ ì§‘ê³„\n",
    "    stats = {\n",
    "        \"Direct Match\": 0,    # DBì— ë°”ë¡œ ìˆìŒ\n",
    "        \"Synonym (Rule)\": 0,  # 1:1 ê·œì¹™ ë§¤í•‘\n",
    "        \"Compound (Rule)\": 0, # 1:N ê·œì¹™ ë¶„í•´\n",
    "        \"Manual Review\": 0,   # ìˆ˜ì‘ì—… í•„ìš”\n",
    "        \"Missing\": 0          # ê·œì¹™ ì—†ìŒ (ìˆ«ì/ì´ë¯¸ì§€ í¬í•¨)\n",
    "    }\n",
    "\n",
    "    subst = rules.get(\"word_substitution\", {})\n",
    "    manual = rules.get(\"manual_review_required\", {})\n",
    "\n",
    "    for t in tokens:\n",
    "        if t in db_keys:\n",
    "            stats[\"Direct Match\"] += 1\n",
    "        elif t in subst:\n",
    "            if len(subst[t]) == 1:\n",
    "                stats[\"Synonym (Rule)\"] += 1\n",
    "            else:\n",
    "                stats[\"Compound (Rule)\"] += 1\n",
    "        elif t in manual:\n",
    "            stats[\"Manual Review\"] += 1\n",
    "        else:\n",
    "            stats[\"Missing\"] += 1\n",
    "\n",
    "    # 3. ê·¸ë˜í”„ ê·¸ë¦¬ê¸°\n",
    "    labels = list(stats.keys())\n",
    "    values = list(stats.values())\n",
    "    colors = ['#4CAF50', '#2196F3', '#9C27B0', '#FF9800', '#9E9E9E'] # ì´ˆë¡, íŒŒë‘, ë³´ë¼, ì£¼í™©, íšŒìƒ‰\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    bars = plt.bar(labels, values, color=colors, edgecolor='black', alpha=0.7)\n",
    "\n",
    "    # ìˆ«ì í‘œì‹œ\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, height, f'{height}', \n",
    "                 ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "\n",
    "    plt.title(f\"External Token Coverage (Total: {len(tokens)})\", fontsize=14)\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"ğŸ“Š [ìƒì„¸ ìˆ˜ì¹˜]\")\n",
    "    for k, v in stats.items():\n",
    "        print(f\" - {k}: {v}\")\n",
    "\n",
    "# ì‹¤í–‰\n",
    "visualize_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53dc2ecc",
   "metadata": {},
   "source": [
    "### Manual Reviewë§Œ ë¶„ë¦¬í•˜ì—¬ CSVë¡œ ì €ì¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62f55a17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âŒ 'rules.json' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# íŒŒì¼ ê²½ë¡œ ì„¤ì • (í•„ìš”ì— ë”°ë¼ ìˆ˜ì •)\n",
    "RULES_JSON_PATH = \"rules.json\"\n",
    "OUTPUT_CSV_PATH = \"manual_review_list.csv\"\n",
    "\n",
    "def export_manual_review_to_csv():\n",
    "    # 1. rules.json ë¡œë“œ\n",
    "    if not Path(RULES_JSON_PATH).exists():\n",
    "        print(f\"âŒ '{RULES_JSON_PATH}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        return\n",
    "\n",
    "    with open(RULES_JSON_PATH, 'r', encoding='utf-8') as f:\n",
    "        rules = json.load(f)\n",
    "\n",
    "    manual_data = rules.get(\"manual_review_required\", {})\n",
    "\n",
    "    if not manual_data:\n",
    "        print(\"âœ… ìˆ˜ì‘ì—… ê²€í† ê°€ í•„ìš”í•œ í•­ëª©ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        return\n",
    "\n",
    "    # 2. ë°ì´í„° ì¶”ì¶œ ë° êµ¬ì¡°í™”\n",
    "    rows = []\n",
    "    for word, info in manual_data.items():\n",
    "        reason = info.get(\"reason\", \"\")\n",
    "        suggested = info.get(\"suggested\", [])\n",
    "        context = info.get(\"context\", \"ë¬¸ë§¥ ì—†ìŒ\")\n",
    "\n",
    "        # ìœ í˜• ë¶„ë¥˜ (ê²€í†  ìš°ì„ ìˆœìœ„ ê²°ì •ì„ ìœ„í•´)\n",
    "        if \"Target not in DB\" in reason:\n",
    "            review_type = \"A_DBë¯¸ì¡´ì¬ (ìˆ˜ì •í•„ìˆ˜)\"\n",
    "        elif \"Uncertain\" in reason:\n",
    "            review_type = \"B_ë¶ˆí™•ì‹¤ (ê²€í† ê¶Œì¥)\"\n",
    "        else:\n",
    "            review_type = \"C_ê¸°íƒ€\"\n",
    "\n",
    "        rows.append({\n",
    "            \"ìœ í˜•\": review_type,\n",
    "            \"ì›ë³¸ ë‹¨ì–´\": word,\n",
    "            \"ì¶”ì²œ ë‹¨ì–´ (ìˆ˜ì •ìš©)\": \", \".join(suggested),  # ë¦¬ìŠ¤íŠ¸ -> ë¬¸ìì—´\n",
    "            \"ì´ìœ \": reason,\n",
    "            \"ë¬¸ë§¥\": context\n",
    "        })\n",
    "\n",
    "    # 3. ë°ì´í„°í”„ë ˆì„ ìƒì„± ë° ì •ë ¬\n",
    "    df = pd.DataFrame(rows)\n",
    "    \n",
    "    # ìœ í˜•ë³„, ë‹¨ì–´ë³„ ì •ë ¬\n",
    "    df = df.sort_values(by=[\"ìœ í˜•\", \"ì›ë³¸ ë‹¨ì–´\"])\n",
    "\n",
    "    # 4. CSV ì €ì¥ (ì—‘ì…€ í˜¸í™˜ ì¸ì½”ë”©)\n",
    "    df.to_csv(OUTPUT_CSV_PATH, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    print(f\"âœ… '{OUTPUT_CSV_PATH}' íŒŒì¼ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "    print(f\"   - ì´ ê²€í†  ëŒ€ìƒ: {len(df)}ê°œ\")\n",
    "    print(f\"   - ì—‘ì…€ì—ì„œ ì—´ì–´ì„œ 'ì¶”ì²œ ë‹¨ì–´ (ìˆ˜ì •ìš©)' ì»¬ëŸ¼ì„ ì˜¬ë°”ë¥¸ DB ë‹¨ì–´ë¡œ ìˆ˜ì •í•˜ì„¸ìš”.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    export_manual_review_to_csv()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
