# -*- coding: utf-8 -*-
#new

"""
Django ë°±ì—”ë“œìš© íŒŒì´í”„ë¼ì¸ ëª¨ë“ˆ

ì—­í• :
- WAV/WEBM ë“± ìŒì„± íŒŒì¼ -> STT(whisper)
- STT í…ìŠ¤íŠ¸ -> Gemini ê¸°ë°˜ í† í° ì¶”ì¶œ(tokens: gloss/image/pause)
- ê°„ë‹¨ ë²„ì „: extract_glosses(text, model) -> gloss ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸ (ê¸°ì¡´ service.py í˜¸í™˜)
- gloss ë¦¬ìŠ¤íŠ¸ -> gloss_id ë§¤í•‘(CSV ì‚¬ì „)
- gloss_id -> ìˆ˜ì–´ ì˜ìƒ(mp4) ê²½ë¡œ ë¦¬ìŠ¤íŠ¸

service.pyì—ì„œ import í•˜ëŠ” ì‹¬ë³¼:
    stt_from_file
    extract_glosses
    to_gloss_ids
    load_gloss_index
    _paths_from_ids
    build_gemini
    MEDIA_ROOT
    now_ts
    OUT_DIR
    _norm
    GEMINI_MODEL
    _local_gloss_rules
"""

import os
import csv
import re
import json
import ast
import unicodedata
import difflib
import wave
import sys
from datetime import datetime
from pathlib import Path
import shutil
import subprocess
import tempfile
import time  # ë””ë²„ê¹…ìš©

import whisper
from dotenv import load_dotenv

from PIL import Image, ImageDraw, ImageFont

# Gemini ë¼ì´ë¸ŒëŸ¬ë¦¬
try:
    import google.generativeai as genai
except Exception:
    genai = None

# Django MEDIA_ROOT ì—°ë™ (ì—†ìœ¼ë©´ ë¡œì»¬ media í´ë” ì‚¬ìš©)
try:
    from django.conf import settings
    MEDIA_ROOT = Path(getattr(settings, "MEDIA_ROOT", "media")).resolve()
except Exception:
    MEDIA_ROOT = Path(__file__).resolve().parent / "media"

# 1. .env ë¡œë“œ
load_dotenv()

# 2. í™˜ê²½ ë³€ìˆ˜
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")

if not GOOGLE_API_KEY:
    print("âš ï¸  [Warn] GOOGLE_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. Gemini ì—†ì´ ë¡œì»¬ ê·œì¹™ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.")

# 3. ê²½ë¡œ ì„¤ì •
ROOT_DIR = Path(__file__).resolve().parent

GLOSS_NEW_DIR = ROOT_DIR / "gloss_new"

DATA_DIR = GLOSS_NEW_DIR / "data"          # backend/pipelines/gloss_new/data
OUT_DIR = GLOSS_NEW_DIR / "snapshots14"

# ì´ ê²½ë¡œë“¤ì€ ë„¤ í”„ë¡œì íŠ¸ êµ¬ì¡°ì— ë§ê²Œ í•œ ë²ˆ í™•ì¸í•´ì¤˜
GLOSS_DICT_PATH = DATA_DIR / "gloss_dictionary_MOCK.csv"

# ê·œì¹™ íŒŒì¼ ë‘ ê°œ ì‚¬ìš©:
# - rules_base.json: ì‚¬ëŒì´ ê´€ë¦¬í•˜ëŠ” ê¸°ë³¸ ê·œì¹™
# - rules.json: í•™ìŠµ/ì¶”ê°€ ê·œì¹™ í¬í•¨ ì‹¤ì œ ìš´ì˜ ê·œì¹™
RULES_PATH = DATA_DIR / "rules.json"        # ì‹¤ì œ ì‚¬ìš© Â· ìë™ ì—…ë°ì´íŠ¸ ëŒ€ìƒ
RULES_BASE_PATH = DATA_DIR / "rules_base.json"

GLOSS_MP4_DIR = Path(
    r"C:\Users\user\Desktop\2025-2-DSCD-KKHH-04-git\backend\pipelines\gloss_new\data\service"
)
# ìˆ˜ì–´ mp4ê°€ ìˆëŠ” ë£¨íŠ¸ í´ë” (í•˜ìœ„ fi, li ë“± í¬í•¨)

VIDEO_OUT_DIR = GLOSS_NEW_DIR / "vd_output"
OUT_DIR.mkdir(exist_ok=True)
VIDEO_OUT_DIR.mkdir(exist_ok=True)

# ğŸ”¹ gloss ë§¤í•‘ ë¡œê·¸ ì €ì¥ í´ë”/íŒŒì¼ ì„¤ì •
LOG_DIR = ROOT_DIR / "gloss_tools"
LOG_DIR.mkdir(parents=True, exist_ok=True)
GLOSS_LOG_FILE = LOG_DIR / "gloss_mapping_log.csv"


# =========================
# rules_base.json + rules.json ìœ í‹¸
# =========================

def _load_json(path: Path) -> dict:
    """
    JSON íŒŒì¼ì„ ì•ˆì „í•˜ê²Œ ì½ì–´ì„œ dictë¡œ ë°˜í™˜.
    íŒŒì¼ì´ ì—†ê±°ë‚˜ í˜•ì‹ì´ ì˜ëª»ë˜ë©´ ë¹ˆ dict ë°˜í™˜.
    """
    if not path.exists():
        return {}
    try:
        with path.open("r", encoding="utf-8") as f:
            return json.load(f)
    except Exception:
        return {}


def _save_json(path: Path, data: dict):
    """
    dictë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥.
    ìƒìœ„ ë””ë ‰í„°ë¦¬ê°€ ì—†ìœ¼ë©´ ìƒì„±í•œë‹¤.
    """
    path.parent.mkdir(parents=True, exist_ok=True)
    with path.open("w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)


def merge_rules() -> dict:
    """
    rules_base.json + rules.jsonì„ í•©ì³ì„œ í•˜ë‚˜ì˜ dictë¡œ ë°˜í™˜.

    êµ¬ì¡° ì˜ˆ:
    {
      "disambiguation_rules": { ... },
      "text_normalization": [ {...}, {...} ]
    }

    - disambiguation_rules: learned(rules.json)ì´ baseë¥¼ ë®ì–´ì”€
    - text_normalization: base + learned ìˆœì„œëŒ€ë¡œ ì´ì–´ ë¶™ì„
    """
    base = _load_json(RULES_BASE_PATH)
    learned = _load_json(RULES_PATH)

    base_dis = base.get("disambiguation_rules", {}) or {}
    learned_dis = learned.get("disambiguation_rules", {}) or {}

    base_norm = base.get("text_normalization", []) or []
    learned_norm = learned.get("text_normalization", []) or []

    return {
        "disambiguation_rules": {
            **base_dis,
            **learned_dis,  # learnedê°€ ìˆìœ¼ë©´ baseë¥¼ ë®ì–´ì”€
        },
        "text_normalization": base_norm + learned_norm,
    }


def append_learned_rule(wrong: str, correct: str):
    """
    wrong â†’ correct ê·œì¹™ì„ rules.json(text_normalization)ì— ì¶”ê°€.
    rules_base.jsonì€ ê±´ë“œë¦¬ì§€ ì•ŠëŠ”ë‹¤.
    """
    wrong = (wrong or "").strip()
    correct = (correct or "").strip()
    if not wrong or not correct:
        return

    data = _load_json(RULES_PATH)
    if not isinstance(data, dict):
        data = {}

    tn_list = data.get("text_normalization", [])
    if not isinstance(tn_list, list):
        tn_list = []

    # ì¤‘ë³µ ë°©ì§€
    for r in tn_list:
        if r.get("wrong") == wrong and r.get("correct") == correct:
            return  # ì´ë¯¸ ë™ì¼ ê·œì¹™ ì¡´ì¬

    tn_list.append({"wrong": wrong, "correct": correct})
    data["text_normalization"] = tn_list
    _save_json(RULES_PATH, data)


# ëª¨ë“ˆ ë¡œë“œ ì‹œ base+learned ê·œì¹™ í•œ ë²ˆ ë¨¸ì§€í•´ì„œ ì „ì—­ìœ¼ë¡œ ë³´ê´€
MERGED_RULES = merge_rules()


def append_normalization_rule(wrong: str, correct: str):
    """
    Django views(add_rule)ì—ì„œ ì‚¬ìš©í•˜ëŠ” wrapper.

    - rules.json(text_normalization)ì— ê·œì¹™ ì¶”ê°€
    - MERGED_RULESë„ ë‹¤ì‹œ ë¨¸ì§€í•´ì„œ ìµœì‹  ìƒíƒœë¡œ ê°±ì‹ 
    """
    global MERGED_RULES
    append_learned_rule(wrong, correct)
    MERGED_RULES = merge_rules()


def apply_text_normalization(text: str, rules: dict | None = None) -> str:
    """
    rules['text_normalization']ì— ìˆëŠ”
    {wrong, correct} ë¦¬ìŠ¤íŠ¸ë¥¼ ìˆœì„œëŒ€ë¡œ ì ìš©í•´ì„œ í…ìŠ¤íŠ¸ ì •ê·œí™”.

    - rulesê°€ Noneì´ë©´ MERGED_RULES ì‚¬ìš©
    - service.pyì—ì„œëŠ” ë³´í†µ apply_text_normalization(clean_text) ì´ë ‡ê²Œë§Œ í˜¸ì¶œí•´ë„ ë¨
    """
    if not text:
        return text

    if rules is None:
        rules = MERGED_RULES

    norm_rules = rules.get("text_normalization", []) or []
    out = text
    for r in norm_rules:
        w = (r.get("wrong") or "").strip()
        c = (r.get("correct") or "").strip()
        if not w or not c:
            continue
        out = out.replace(w, c)
    return out


def log_gloss_mapping(
    gloss_list,
    gloss_ids,
    gloss_labels,
    text=None,
    mode=None,
    session_id=None,
    ts=None,
    only_mismatch=True,
):
    """
    gloss / gloss_ids / gloss_labels ë§¤í•‘ ê²°ê³¼ë¥¼ CSVë¡œ ê¸°ë¡.
    only_mismatch=Trueë©´, gloss != gloss_labels ìˆëŠ” ê²½ìš°ë§Œ ê¸°ë¡.
    """
    if gloss_list is None:
        gloss_list = []
    if gloss_ids is None:
        gloss_ids = []
    if gloss_labels is None:
        gloss_labels = []

    # mismatch ì—¬ë¶€ ì²´í¬
    has_mismatch = any(
        (g != l) for g, l in zip(gloss_list, gloss_labels)
    )

    # mismatchë§Œ ê¸°ë¡í•˜ê³  ì‹¶ìœ¼ë©´
    if only_mismatch and not has_mismatch:
        return

    if ts is None:
        ts = datetime.now().strftime("%Y%m%d_%H%M%S")

    row = {
        "timestamp": ts,
        "session_id": session_id or "",
        "mode": mode or "",
        "text": text or "",
        "gloss": "|".join(gloss_list),
        "gloss_ids": "|".join(gloss_ids),
        "gloss_labels": "|".join(gloss_labels),
        "has_mismatch": "1" if has_mismatch else "0",
    }

    file_exists = GLOSS_LOG_FILE.exists()
    with GLOSS_LOG_FILE.open("a", newline="", encoding="utf-8") as f:
        writer = csv.DictWriter(
            f,
            fieldnames=[
                "timestamp",
                "session_id",
                "mode",
                "text",
                "gloss",
                "gloss_ids",
                "gloss_labels",
                "has_mismatch",
            ],
        )
        if not file_exists:
            writer.writeheader()
        writer.writerow(row)


print("ğŸ”„ NEW pipeline.py loaded")
print("ğŸ“ GLOSS_DICT_PATH   =", GLOSS_DICT_PATH)
print("ğŸ“ RULES_BASE_PATH   =", RULES_BASE_PATH)
print("ğŸ“ RULES_PATH        =", RULES_PATH)
print("ğŸ“ GLOSS_MP4_DIR     =", GLOSS_MP4_DIR)
print("ğŸ“ GLOSS_LOG_FILE    =", GLOSS_LOG_FILE)

# 4. ëª¨ë¸/ì˜¤ë””ì˜¤ ì„¤ì •
GEMINI_MODEL_NAME = "models/gemini-2.5-flash"
WHISPER_MODEL_NAME = "small"
WHISPER_LANG = "ko"

ALWAYS_RETURN_ID = True  # ë§¤í•‘ ì‹¤íŒ¨ ì‹œì—ë„ ìœ ì‚¬ë„ ê¸°ë°˜ìœ¼ë¡œ ID í•˜ë‚˜ëŠ” ì„ íƒ

# ì „ì—­ ìºì‹œ
GEMINI_MODEL = None
_WHISPER_MODEL = None
WHISPER_LOAD_MS = None

# ======================================================================
# ê³µí†µ ìœ í‹¸
# ======================================================================

def _norm(s: str) -> str:
    """ì „ê°/ë°˜ê° í†µì¼ + ì–‘ ë ê³µë°± ì œê±° + ë‚´ë¶€ ë‹¤ì¤‘ ê³µë°±ì„ 1ì¹¸ìœ¼ë¡œ ì¶•ì†Œ."""
    s = unicodedata.normalize("NFKC", s or "").strip()
    s = re.sub(r"\s+", " ", s)
    return s


def _first_word(phrase: str) -> str:
    """ë¬¸ì¥ì—ì„œ ì²« ë‹¨ì–´ë§Œ ì¶”ì¶œ(ê¸€ë¡œìŠ¤ëŠ” ë‹¨ì–´ 1ê°œë¼ëŠ” ì „ì œ ìœ ì§€ìš©)."""
    s = _norm(phrase)
    return s.split()[0] if s else ""


def _nospace(s: str) -> str:
    """ê³µë°±/ê¸°í˜¸ ì œê±° í›„ ë¹„êµìš© í‚¤ ìƒì„±."""
    return re.sub(r"[^\wê°€-í£]", "", re.sub(r"\s+", "", _norm(s)))


def now_ts() -> str:
    """í˜„ì¬ ì‹œê°ì„ YYYYmmdd_HHMMSS ë¬¸ìì—´ë¡œ ë°˜í™˜."""
    return datetime.now().strftime("%Y%m%d_%H%M%S")


# ======================================================================
# STT (íŒŒì¼ ê¸°ë°˜) - service.pyì—ì„œ ì‚¬ìš©
# ======================================================================

def _get_whisper_model():
    global _WHISPER_MODEL, WHISPER_LOAD_MS
    if _WHISPER_MODEL is None:
        print(f"[Whisper] loading model: {WHISPER_MODEL_NAME}")
        t0 = time.perf_counter()  # ğŸ”¹ ë¡œë”© ì‹œì‘ ì‹œê°„
        try:
            # CPU ê¸°ì¤€ìœ¼ë¡œ ëª…ì‹œ
            _WHISPER_MODEL = whisper.load_model(WHISPER_MODEL_NAME, device="cpu")
            WHISPER_LOAD_MS = (time.perf_counter() - t0) * 1000.0
            print(
                f"[Whisper Init] whisper.load_model('{WHISPER_MODEL_NAME}') "
                f"{WHISPER_LOAD_MS:.1f} ms"
            )
        except Exception as e:
            print(f"[Whisper] ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            raise
    return _WHISPER_MODEL


def stt_from_file(audio_path: str) -> str:
    """
    ì„œë²„ì—ì„œ íŒŒì¼ ê²½ë¡œë¥¼ ë°›ì•„ STT ìˆ˜í–‰ í›„ í…ìŠ¤íŠ¸ ë°˜í™˜.
    - í˜¸ì¶œì€ 1ë²ˆë§Œ.
    - ë‹¨, no_speech_threshold / logprob_thresholdë¥¼ ì™„í™”í•´ì„œ
      ì§§ì€ ì¸ì‚¬ ê°™ì€ ë¬¸ì¥ì´ ë¹ˆ ë¬¸ìì—´ë¡œ ë‚ ì•„ê°€ëŠ” ê±¸ ì¤„ì¸ë‹¤.
    """
    model = _get_whisper_model()
    t0 = time.perf_counter()
    res = model.transcribe(
        str(audio_path),
        language=WHISPER_LANG,
        fp16=False,              # CPUë©´ í•­ìƒ False
        temperature=0.0,         # ëœë¤ì„± ìµœì†Œí™”
        beam_size=1,
        best_of=1,
        condition_on_previous_text=False,

        # ğŸ”½ ì—¬ê¸° ì„¸ ê°œê°€ í•µì‹¬
        #    - "ë¬´ìŒ ê°™ë‹¤"ë¼ê³  íŒë‹¨í•˜ëŠ” ê¸°ì¤€ì„ ë” ëŠìŠ¨í•˜ê²Œ
        no_speech_threshold=0.05,       # ê¸°ë³¸ê°’ë³´ë‹¤ â†“ (ë§ ì¡°ê¸ˆë§Œ ìˆì–´ë„ ì¸ì‹)
        logprob_threshold=-2.0,         # ë„ˆë¬´ ë¹¡ì„¼ í•„í„° ì™„í™”
        compression_ratio_threshold=2.0 # ì¡ìŒ í•„í„°ë„ ì•½í•˜ê²Œ
    )
    t1 = time.perf_counter()
    print(f"[STT inner] whisper.transcribe only: {t1 - t0:.2f} sec for {audio_path}")

    stt_text = _norm(res.get("text") or "")
    print(f"[STT] {audio_path} -> \"{stt_text}\"")
    return stt_text


# ======================================================================
# Gemini ì„¤ì • ë° í† í° ì¶”ì¶œ (ê³ ê¸‰ ë²„ì „)
# ======================================================================

def build_gemini():
    """
    Gemini ëª¨ë¸ ìƒì„±.
    - GOOGLE_API_KEY ì—†ìœ¼ë©´ None ë°˜í™˜ (service.pyì—ì„œ None ì²´í¬ í›„ ë¡œì»¬ ê·œì¹™ ì‚¬ìš© ê°€ëŠ¥).
    """
    if not GOOGLE_API_KEY or genai is None:
        return None

    genai.configure(api_key=GOOGLE_API_KEY)

    sys_prompt = f"""
    ë‹¹ì‹ ì€ 'ì²­ê°ì¥ì• ì¸ì„ ìœ„í•œ ì „ë¬¸ ìˆ˜ì–´(KSL) í†µì—­ì‚¬'ì…ë‹ˆë‹¤. 
    ì…ë ¥ëœ ë¬¸ì¥ì„ ë‹¨ìˆœ ë²ˆì—­í•˜ì§€ ë§ê³ , 'ë†ë¬¸í™”(Deaf Culture)'ì™€ 'í•œêµ­ìˆ˜ì–´ ë¬¸ë²•'ì— ë§ì¶° ì˜ë¯¸ë¥¼ ì¬êµ¬ì„±(Paraphrasing)í•˜ì‹­ì‹œì˜¤.

    [í•µì‹¬ ì‘ì—… ì›ì¹™]
    1. ìˆ˜ì§€í•œêµ­ì–´(SK) ê¸ˆì§€: í•œêµ­ì–´ì˜ ì–´ìˆœì´ë‚˜ ë¬¸ë²• ìš”ì†Œ(ì¡°ì‚¬, ì–´ë¯¸)ë¥¼ ê·¸ëŒ€ë¡œ ë”°ë¼ê°€ì§€ ë§ˆì‹­ì‹œì˜¤.
    2. ì˜ë¯¸ ì¤‘ì‹¬ ë²ˆì—­: ë¬¸ì¥ì˜ 'í•µì‹¬ ì˜ë„'ë¥¼ íŒŒì•…í•˜ì—¬ ê°€ì¥ ì§ê´€ì ì¸ ë‹¨ì–´ë“¤ì˜ ë‚˜ì—´ë¡œ ë°”ê¾¸ì‹­ì‹œì˜¤.
    3. ë©”íƒ€ ë°œí™” ì‚­ì œ: "ì•ˆë‚´í•´ ë“œë¦¬ê² ìŠµë‹ˆë‹¤", "ë§ì”€ë“œë¦¬ìë©´" ë“± ì •ë³´ê°€ê°€ ì—†ëŠ” ë©˜íŠ¸ëŠ” ê³¼ê°íˆ ì‚­ì œí•˜ì‹­ì‹œì˜¤.
       - ë‹¨, 'ì•ˆë…•í•˜ì„¸ìš”', 'ë°˜ê°‘ìŠµë‹ˆë‹¤', 'ê³ ë§™ìŠµë‹ˆë‹¤(ê°ì‚¬í•©ë‹ˆë‹¤)', 'ìˆ˜ê³ í•˜ì…¨ìŠµë‹ˆë‹¤' ë“± ì‚¬íšŒì  ê´€ê³„ë¥¼ ë§ºëŠ” ì¸ì‚¬ë§ì€ ì‚­ì œí•˜ì§€ ë§ê³  ë°˜ë“œì‹œ ìˆ˜ì–´ ë‹¨ì–´ë¡œ ë³€í™˜í•˜ì‹­ì‹œì˜¤.
    4. í•œêµ­ì–´ ì „ìš© ì¶œë ¥ (Korean Only): 
       - ê²°ê³¼ JSONì˜ 'text' í•„ë“œ ê°’ì—ëŠ” 'ë°˜ë“œì‹œ í•œêµ­ì–´ ë˜ëŠ” ìˆ«ì'ë§Œ ë“¤ì–´ê°€ì•¼ í•©ë‹ˆë‹¤.
       - ì˜ì–´ ë‹¨ì–´(ì˜ˆ: 'Limit', 'Bank')ê°€ í¬í•¨ë˜ë©´ ë¬´ì¡°ê±´ í•œêµ­ì–´ ëœ»ìœ¼ë¡œ ë²ˆì—­í•˜ì—¬ ì¶œë ¥í•˜ì‹­ì‹œì˜¤.
    5. ê³ ìœ ëª…ì‚¬ ë° ìƒí’ˆëª… ì²˜ë¦¬ (Image Mapping): 
       - ì‚¬ëŒì˜ ì´ë¦„(ì„±ëª…), ë‚¯ì„  ì§€ëª…, ë¸Œëœë“œëª…, ê·¸ë¦¬ê³  'êµ¬ì²´ì ì¸ ê¸ˆìœµ ìƒí’ˆëª…'ì€ ìˆ˜ì–´ë¡œ ì–µì§€ë¡œ ë²ˆì—­í•˜ê±°ë‚˜ ìª¼ê°œì§€ ë§ê³  ë°˜ë“œì‹œ ì „ì²´ë¥¼ í•˜ë‚˜ì˜ í…ìŠ¤íŠ¸ ì´ë¯¸ì§€ë¡œ ë³€í™˜í•˜ì‹­ì‹œì˜¤.
       - ì˜ì–´ì™€ í•œê¸€ì´ ì„ì—¬ ìˆì–´ë„ í•©ì³ì„œ í•˜ë‚˜ì˜ ì´ë¯¸ì§€ë¡œ ë§Œë“œì‹­ì‹œì˜¤.
       - ì˜ˆ: "ì €ëŠ” ê¹€ë™í˜¸ì…ë‹ˆë‹¤." -> '[ì €], [PAUSE], [ê¹€ë™í˜¸(image)]'
       - ì˜ˆ: "KBë‚˜ë¼ì‚¬ë‘ì ê¸ˆ ìƒí’ˆ" -> '[KBë‚˜ë¼ì‚¬ë‘ì ê¸ˆ(image)], [ìƒí’ˆ]'


    [ë¬¸ë²• ë° êµ¬ì¡° ê·œì¹™ (Strict Rules)]
    
    1. í™”ì œ-ì„œìˆ  êµ¬ì¡° (Topic-Comment):
       - ë¬¸ì¥ ë§¨ ì•ì— [ì‹œê°„] -> [ì¥ì†Œ] -> [í™”ì œ(Topic)]ë¥¼ ë°°ì¹˜í•˜ì‹­ì‹œì˜¤.
       - í™”ì œì™€ ì„œìˆ ë¶€ ì‚¬ì´ì—ëŠ” ë°˜ë“œì‹œ `type: "pause"`ë¥¼ ì‚½ì…í•˜ì—¬ ì‹œê°ì  í˜¸í¡ì„ ì£¼ì‹­ì‹œì˜¤.
       - ì˜ˆ: "ì–´ì œ ì§‘ì—ì„œ ë°¥ì„ ë¨¹ì—ˆë‹¤" -> [ì–´ì œ], [ì§‘], [PAUSE], [ë°¥], [ë¨¹ë‹¤]
    
    2. ìˆ˜ëŸ‰ì‚¬ ë° ìˆ˜ì‹ì–´ í›„ì¹˜ (Post-position):
       - [ìˆ˜ëŸ‰]: 'í•œ ì‚¬ëŒ', 'ë‘ ê°œì˜ ê³„ì¢Œ'ëŠ” ë°˜ë“œì‹œ [ëª…ì‚¬] + [ìˆ˜ëŸ‰] ìˆœì„œë¡œ ë³€ê²½í•˜ì‹­ì‹œì˜¤. 
         -> "í•œ ì‚¬ëŒ" (X) -> [ì‚¬ëŒ], [1ëª…(ì´ë¯¸ì§€)] (O)
       - [ë¶€ì •ì–´]: ì„œìˆ ì–´ ë’¤ì— ìœ„ì¹˜ì‹œí‚µë‹ˆë‹¤. (ì˜ˆ: [ê°€ë‹¤], [ì•ˆí•˜ë‹¤])
       - [í˜•ìš©ì‚¬]: ëª…ì‚¬ ë’¤ì— ìœ„ì¹˜ì‹œí‚µë‹ˆë‹¤. (ì˜ˆ: [ë”¸], [ì˜ˆì˜ë‹¤])

    3. ìˆ«ì ë° ë‹¨ìœ„ ì²˜ë¦¬ (ì´ë¯¸ì§€í™”):
       - ì˜¤ì¸ì‹ ë°©ì§€ë¥¼ ìœ„í•´ ìˆ«ìê°€ í¬í•¨ëœ ëª¨ë“  í‘œí˜„ì€ í…ìŠ¤íŠ¸ ì´ë¯¸ì§€ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
       - ê´€í˜•ì‚¬ 'í•œ, ë‘, ì„¸'ëŠ” ë°˜ë“œì‹œ ì•„ë¼ë¹„ì•„ ìˆ«ì '1, 2, 3'ìœ¼ë¡œ ë³€í™˜í•˜ì‹­ì‹œì˜¤.
       - % (í¼ì„¼íŠ¸): '[{{ "text": "3.5", "type": "image" }}, {{ "text": "í¼ì„¼íŠ¸", "type": "gloss" }}]'
       - %p (í¼ì„¼íŠ¸ í¬ì¸íŠ¸): '[{{ "text": "0.5", "type": "image" }}, {{ "text": "í¼ì„¼íŠ¸", "type": "gloss" }}, {{ "text": "í¬ì¸íŠ¸", "type": "gloss" }}]'
       - ì—° ì´ìœ¨: 'ì—°'ì€ `[1ë…„]` ìˆ˜ì–´ë¡œ, ì´ìœ¨ì€ '[í¼ì„¼íŠ¸]'ë¡œ ì²˜ë¦¬.

    4. ì–´íœ˜ ë‹¨ìˆœí™” (Vocabulary Simplification):
       - ì–´ë ¤ìš´ í•œìì–´, ì „ë¬¸ ìš©ì–´ëŠ” ê¸°ì´ˆì ì¸ ìˆ˜ì–´ ë‹¨ì–´ì˜ ì¡°í•©ìœ¼ë¡œ í’€ì–´ì„œ ì„¤ëª…í•˜ì‹­ì‹œì˜¤.
       - ì˜ˆ: "ì£¼íƒë‹´ë³´ëŒ€ì¶œ" -> '[ì§‘]', '[ë§¡ê¸°ë‹¤]', '[ëˆ]', '[ë¹Œë¦¬ë‹¤]'
       - ì˜ˆ: "ìš°ëŒ€ê¸ˆë¦¬" -> '[íŠ¹ë³„]', '[ì´ì]'

    [Few-shot Examples]

    ì…ë ¥: "ì´ ìƒí’ˆì€ í•œ ì‚¬ëŒë‹¹ í•˜ë‚˜ì˜ ê³„ì¢Œë§Œ ê°œì„¤ ê°€ëŠ¥í•©ë‹ˆë‹¤."
    ì¶œë ¥:
    {{
        "cleaned": "ìƒí’ˆ ì´ê²ƒ ì‚¬ëŒ 1ëª… ê³„ì¢Œ 1ê°œ ê°œì„¤ ê°€ëŠ¥",
        "tokens": [
            {{ "text": "ìƒí’ˆ", "type": "gloss" }},
            {{ "text": "ì´ê²ƒ", "type": "gloss" }},
            {{ "text": "PAUSE", "type": "pause" }},
            {{ "text": "ì‚¬ëŒ", "type": "gloss" }},
            {{ "text": "1ëª…", "type": "image" }},
            {{ "text": "ê³„ì¢Œ", "type": "gloss" }},
            {{ "text": "1ê°œ", "type": "image" }},
            {{ "text": "ê°œì„¤", "type": "gloss" }},
            {{ "text": "ê°€ëŠ¥", "type": "gloss" }}
        ]
    }}

    ì…ë ¥: "ê¸ˆë¦¬ëŠ” ì—° 3.5%í¬ì¸íŠ¸ ìš°ëŒ€ ì ìš©ë©ë‹ˆë‹¤."
    ì¶œë ¥:
    {{
        "cleaned": "ê¸ˆë¦¬ 1ë…„ 3.5 í¼ì„¼íŠ¸ ì ìˆ˜ íŠ¹ë³„ ì ìš©",
        "tokens": [
            {{ "text": "ê¸ˆë¦¬", "type": "gloss" }},
            {{ "text": "PAUSE", "type": "pause" }},
            {{ "text": "1ë…„", "type": "gloss" }},
            {{ "text": "3.5", "type": "image" }},
            {{ "text": "í¼ì„¼íŠ¸", "type": "gloss" }},
            {{ "text": "ì ìˆ˜", "type": "gloss" }},
            {{ "text": "íŠ¹ë³„", "type": "gloss" }},
            {{ "text": "ì ìš©", "type": "gloss" }}
        ]
    }}
    5. ë²”ìœ„ í‘œí˜„ (Range):
       - 'ì´ìƒ/ì´í•˜/ì´ˆê³¼/ë¯¸ë§Œ'ì€ ì˜¤ì—­ ë°©ì§€ë¥¼ ìœ„í•´ ë°˜ë“œì‹œ 'ë¶€í„°(~ë¶€í„°)'ì™€ 'ê¹Œì§€(~ê¹Œì§€)'ë¡œ ë³€í™˜í•˜ì‹­ì‹œì˜¤.
       - ì…ë ¥: "2.5% ì´ìƒ" -> '[{{ "text": "2.5", "type": "image" }}, {{ "text": "í¼ì„¼íŠ¸", "type": "gloss" }}, {{ "text": "ë¶€í„°", "type": "gloss" }}]'
       - ì…ë ¥: "3.5% ì´í•˜" -> '[{{ "text": "3.5", "type": "image" }}, {{ "text": "í¼ì„¼íŠ¸", "type": "gloss" }}, {{ "text": "ê¹Œì§€", "type": "gloss" }}]'
       - ì…ë ¥: "18ì„¸~30ì„¸" -> '[{{ "text": "18ì„¸", "type": "image" }}, {{ "text": "ë¶€í„°", "type": "gloss" }}, {{ "text": "30ì„¸", "type": "image" }}, {{ "text": "ê¹Œì§€", "type": "gloss" }}]'

    [ì¶œë ¥ í¬ë§· (JSON Only)]
    ë°˜ë“œì‹œ JSON í˜•ì‹ë§Œ ì¶œë ¥í•˜ì„¸ìš”.
    """

    model = genai.GenerativeModel(
        GEMINI_MODEL_NAME,
        system_instruction=sys_prompt,
        generation_config={
            "response_mime_type": "application/json",
            "temperature": 0.2,
        },
    )
    return model


def _get_gemini_model():
    global GEMINI_MODEL
    if GEMINI_MODEL is None:
        GEMINI_MODEL = build_gemini()
    return GEMINI_MODEL


def extract_tokens(text: str, model=None) -> list[dict]:
    """
    ë¬¸ì¥ì„ ë¶„ì„í•˜ì—¬ í† í° ë¦¬ìŠ¤íŠ¸(dict) ë°˜í™˜.
    ë°˜í™˜ ì˜ˆì‹œ: [{'text': 'ë‚˜ì´', 'type': 'gloss'}, {'text': '18ì„¸', 'type': 'image'}]
    """
    clean = _norm(text)
    if not clean:
        return []

    if model is None:
        model = _get_gemini_model()

    # 1) Gemini ì‚¬ìš©
    if model:
        try:
            parts = [{"role": "user", "parts": [clean]}]
            resp = model.generate_content(parts)

            try:
                obj = json.loads(resp.text)
            except Exception:
                m = re.search(r"\{.*\}", resp.text, re.DOTALL)
                if m:
                    obj = json.loads(m.group())
                else:
                    obj = {}

            tokens = obj.get("tokens", [])
            if isinstance(tokens, list):
                # ìµœì†Œí•œ text/type êµ¬ì¡°ë§Œ ë³´ì¥
                out = []
                for t in tokens:
                    if not isinstance(t, dict):
                        continue
                    txt = _first_word(t.get("text", ""))
                    if not txt:
                        continue
                    ttype = t.get("type", "gloss")
                    out.append({"text": txt, "type": ttype})
                if out:
                    return out

        except Exception as e:
            print(f"[Gemini Error] {e}")

    # 2) ë¡œì»¬ í´ë°±: ìˆ«ì + ë‹¨ìœ„ ë˜ëŠ” í•œê¸€/ì˜ë¬¸ ë‹¨ì–´ë¥¼ ì „ë¶€ glossë¡œ ì²˜ë¦¬
    tokens = re.findall(r"\d+(?:ì–µì›|ì–µ\s*ì›|ê°œì›”|ë…„|ì„¸|%)|[ê°€-í£A-Za-z]+", clean)
    return [{"text": _first_word(t), "type": "gloss"} for t in tokens if _first_word(t)]


def extract_glosses(text: str, model=None) -> list[str]:
    """
    service.py í˜¸í™˜ìš© ê°„ë‹¨ ì¸í„°í˜ì´ìŠ¤:
    - ê¸°ì¡´ ë²„ì „ì²˜ëŸ¼ 'ê¸€ë¡œìŠ¤ ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸'ë§Œ ë°˜í™˜.
    - ë‚´ë¶€ì ìœ¼ë¡œëŠ” extract_tokensë¥¼ ì‚¬ìš©í•˜ì§€ë§Œ,
      type == 'gloss' ì¸ ê²ƒë§Œ ì¶”ë ¤ì„œ ë°˜í™˜.
    """
    tokens = extract_tokens(text, model=model)
    gloss_list = [
        t["text"] for t in tokens
        if isinstance(t, dict) and t.get("type", "gloss") == "gloss" and t.get("text")
    ]
    return gloss_list


# ======================================================================
# Gloss ì‚¬ì „ ë¡œë“œ ë° ë§¤í•‘
# ======================================================================

VIDEO_PATH_INDEX = {}


def build_video_index(root_dir: Path):
    """
    í•˜ìœ„ í´ë” í¬í•¨ ëª¨ë“  mp4 íŒŒì¼ì„ ê²€ìƒ‰í•˜ì—¬
    { "íŒŒì¼ID": "ì „ì²´ê²½ë¡œ" } í˜•íƒœì˜ ì§€ë„ë¥¼ ë§Œë“¦.
    """
    global VIDEO_PATH_INDEX
    print(f"ğŸ“‚ ì˜ìƒ íŒŒì¼ ì¸ë±ì‹± ì¤‘... ({root_dir})")

    count = 0
    for path in root_dir.rglob("*.mp4"):
        file_id = path.stem
        VIDEO_PATH_INDEX[file_id] = str(path.resolve())
        count += 1

    print(f"âœ… ì´ {count}ê°œì˜ ì˜ìƒ íŒŒì¼ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.")


# ëª¨ë“ˆ ë¡œë“œ ì‹œ í•œ ë²ˆ ì¸ë±ìŠ¤ êµ¬ì¶•
try:
    if GLOSS_MP4_DIR.exists():
        build_video_index(GLOSS_MP4_DIR)
    else:
        print(f"âš ï¸ GLOSS_MP4_DIRê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {GLOSS_MP4_DIR}")
except Exception as e:
    print(f"âš ï¸ build_video_index ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")


def load_gloss_index(csv_path: Path | str | None = None) -> dict:
    """
    ê¸€ë¡œìŠ¤ ì‚¬ì „ì„ ë¡œë“œí•´ ê²€ìƒ‰ìš© ì¸ë±ìŠ¤ë¥¼ ë§Œë“ ë‹¤.
    - csv_pathë¥¼ ì•ˆ ë„˜ê¸°ë©´ ê¸°ë³¸ìœ¼ë¡œ GLOSS_DICT_PATH ì‚¬ìš©
      (service.pyì—ì„œ load_gloss_index() í˜¸ì¶œí•˜ëŠ” ê²ƒê³¼ í˜¸í™˜)
    """
    if csv_path is None:
        csv_path = GLOSS_DICT_PATH

    csv_path = Path(csv_path)

    rows, exact = [], {}
    id_to_word = {}

    with open(csv_path, "r", encoding="utf-8-sig") as f:
        rdr = csv.DictReader(f)
        headers = [h.strip().lower() for h in (rdr.fieldnames or [])]

        def pick(*cands):
            for c in cands:
                if c in headers:
                    return c
            return None

        h_id = pick("gloss_id", "id", "gid")
        h_ko = pick(
            "korean_meanings", "korean", "ko",
            "meaning_ko", "ko_meanings", "korean_meaning",
        )
        h_cat1 = pick("cat_1", "category_1", "category")

        if not h_id or not h_ko:
            raise RuntimeError(f"[Gloss] í—¤ë” ê°ì§€ ì‹¤íŒ¨: {headers}")

        for row in rdr:
            gid = (row.get(h_id) or "").strip()
            cell = (row.get(h_ko) or "").strip()
            cat1 = (row.get(h_cat1) or "").strip() if h_cat1 else ""

            if not gid or not cell:
                continue

            try:
                obj = ast.literal_eval(cell)
                if isinstance(obj, (list, tuple)):
                    terms = [str(x) for x in obj]
                else:
                    terms = [str(obj)]
            except Exception:
                terms = [cell]

            if terms:
                id_to_word[gid] = terms[0]

            for term in terms:
                term = _norm(term)
                if not term:
                    continue
                term_ns = _nospace(term)
                token_cnt = len(term.split())
                char_len = len(term_ns)
                rows.append({
                    "gid": gid,
                    "term": term,
                    "term_ns": term_ns,
                    "token_cnt": token_cnt,
                    "char_len": char_len,
                    "cat_1": cat1,
                })
                exact.setdefault(term_ns, gid)

    if not rows:
        raise RuntimeError("[Gloss] ì‚¬ì „ì— ìœ íš¨í•œ í•­ëª©ì´ ì—†ìŠµë‹ˆë‹¤.")

    rows.sort(key=lambda x: 0 if "ì „ë¬¸ìš©ì–´" in (x.get("cat_1") or "") else 1)

    print(f"[Gloss] indexed rows={len(rows)}, exact_keys={len(exact)}")
    return {"rows": rows, "exact": exact, "id_to_word": id_to_word}


def map_one_word_to_id(word: str, index: dict, blacklist: list | None = None) -> str | None:
    if not word or not index:
        return None
    if blacklist is None:
        blacklist = []

    rows, exact = index["rows"], index["exact"]
    w = _first_word(word)
    wns = _nospace(w)
    if not wns:
        return None

    gid = exact.get(wns)
    if gid and int(gid) not in blacklist:
        return gid

    cands = [r for r in rows if wns in r["term_ns"] and int(r["gid"]) not in blacklist]
    if cands:
        cands.sort(key=lambda r: (r["token_cnt"], r["char_len"], r["term"], r["gid"]))
        return cands[0]["gid"]

    best_gid, best_sc = None, 0.0
    for r in rows:
        if int(r["gid"]) in blacklist:
            continue
        sc = difflib.SequenceMatcher(None, wns, r["term_ns"]).ratio()
        if sc > best_sc:
            best_sc, best_gid = sc, r["gid"]

    if ALWAYS_RETURN_ID and best_gid:
        return best_gid
    return None


def to_gloss_ids(gloss_list: list[str], index: dict) -> list[str]:
    """
    gloss_list: ["ìë™ì´ì²´", "ê°’", "gloss:ìë™ì´ì²´", "image:1ë…„", ...] ë“±
      - "image:" í† í°ì€ ì—¬ê¸°ì„œ ì²˜ë¦¬í•˜ì§€ ì•ŠìŒ (service.pyì—ì„œ generate_image_video)
      - "gloss:" ì ‘ë‘ì–´ëŠ” ë–¼ê³  ìˆœìˆ˜ í…ìŠ¤íŠ¸ë¡œë§Œ ID ë§¤í•‘
    index: { "ìë™ì´ì²´": "100123", ... }

    ë°˜í™˜: ì¤‘ë³µ ì œê±°ëœ gloss_id ë¦¬ìŠ¤íŠ¸(ì…ë ¥ ìˆœì„œ ë³´ì¡´)
    """
    out: list[str] = []
    seen: set[str] = set()

    for raw in (gloss_list or []):
        if raw is None:
            continue

        g = str(raw).strip()
        if not g:
            continue

        # 1) ì ‘ë‘ì–´ ì •ë¦¬
        if g.startswith("image:"):
            # image í† í°ì€ ì—¬ê¸°ì„œ ID ë³€í™˜í•˜ì§€ ì•ŠìŒ
            print(f"[to_gloss_ids] skip image token: {g!r}")
            continue

        if g.startswith("gloss:"):
            g_clean = g[len("gloss:"):].strip()
        else:
            g_clean = g

        if not g_clean:
            continue

        # 2) ì‹¤ì œ ID ë§¤í•‘
        gid = map_one_word_to_id(g_clean, index)
        if not gid:
            print(f"[to_gloss_ids] no id for gloss='{g_clean}' (from {g!r})")
            continue

        gid_str = str(gid)
        if gid_str not in seen:
            out.append(gid_str)
            seen.add(gid_str)

    return out


def decompose_compound_word(token: str, valid_keys: dict) -> list[str] | None:
    if len(token) < 2:
        return None
    for i in range(1, len(token)):
        part1 = token[:i]
        part2 = token[i:]
        if part1 in valid_keys and part2 in valid_keys:
            return [part1, part2]
    return None


def resolve_gloss_token(token_text, original_sentence, rules, db_index):
    """
    ê³ ê¸‰ ê·œì¹™ ê¸°ë°˜ í† í° -> gloss_id ë§¤í•‘ í•¨ìˆ˜.
    Djangoì—ì„œë„ ì‚¬ìš©í•  ìˆ˜ ìˆê²Œ ë‚¨ê²¨ë‘  (service.pyì—ì„œ ì›í•˜ë©´ ì‚¬ìš©).
    """
    final_ids = []
    resolved_logs = []

    id_map = db_index.get("id_to_word", {})

    blacklist = rules.get("blacklist", [])
    sub_list = rules.get("word_substitution", {}).get(token_text, [token_text])

    for sub in sub_list:
        target_ids = []
        method = "unknown"

        if sub in rules.get("fixed_mappings", {}):
            target_ids.append(rules["fixed_mappings"][sub])
            method = "fixed_rule"

        elif sub in rules.get("disambiguation_rules", {}):
            rule = rules["disambiguation_rules"][sub]
            found = False
            for case in rule["cases"]:
                for kw in case["keywords"]:
                    if kw in original_sentence:
                        target_ids.append(case["target_id"])
                        found = True
                        method = f"context({kw})"
                        break
                if found:
                    break
            if not found:
                target_ids.append(rule["default_id"])
                method = "context_default"

        else:
            gid = map_one_word_to_id(sub, db_index, blacklist)
            if gid:
                target_ids.append(gid)
                method = "exact/similarity"
            else:
                decomposed = decompose_compound_word(sub, db_index["exact"])
                if decomposed:
                    for part in decomposed:
                        part_id = map_one_word_to_id(part, db_index, blacklist)
                        if part_id:
                            target_ids.append(part_id)
                    method = f"decomposed({decomposed})"

        if target_ids:
            final_ids.extend(target_ids)

            real_words = []
            for tid in target_ids:
                rw = id_map.get(str(tid), "UnknownID")
                real_words.append(rw)

            resolved_logs.append({
                "token": sub,
                "resolved_word": real_words,
                "ids": target_ids,
                "method": method,
            })

    return final_ids, resolved_logs


def _paths_from_ids(gloss_ids):
    """
    gloss_id ë¦¬ìŠ¤íŠ¸ë¥¼ ë°›ì•„ ë¯¸ë¦¬ ë§Œë“¤ì–´ë‘” ì§€ë„(VIDEO_PATH_INDEX)ì—ì„œ ê²½ë¡œë¥¼ ì°¾ìŒ.
    - ì—¬ê¸°ë¡œ ë“¤ì–´ì˜¤ëŠ” ê°’ì€ ì›ì¹™ìƒ "100123" ê°™ì€ ìˆœìˆ˜ IDì—¬ì•¼ í•¨.
    - í˜¹ì‹œ 'gloss:...', 'image:...'ê°€ ì„ì—¬ ë“¤ì–´ì™€ë„ ê²½ë¡œë¡œ ì‚¬ìš©í•˜ì§€ ì•Šê³  ìŠ¤í‚µ.
    """
    paths, missing = [], []
    for gid in gloss_ids or []:
        gid_str = str(gid).strip()
        if not gid_str:
            continue

        # ë°©ì–´ ì½”ë“œ: ì˜ëª» ë“¤ì–´ì˜¨ ì ‘ë‘ì–´ í† í°ì€ ë¬´ì‹œ
        if gid_str.startswith("image:"):
            print(f"[paths_from_ids] skip image token in gloss_ids: {gid_str!r}")
            continue
        if gid_str.startswith("gloss:"):
            print(f"[paths_from_ids] unexpected gloss: prefix in gloss_ids: {gid_str!r}")
            # í•„ìš”í•˜ë©´ ì—¬ê¸°ì„œ ì ‘ë‘ì–´ ë–¼ê³  ë‹¤ì‹œ VIDEO_PATH_INDEX ì¡°íšŒí•´ë„ ë¨
            gid_str = gid_str[len("gloss:"):].strip()
            if not gid_str:
                continue

        if gid_str in VIDEO_PATH_INDEX:
            paths.append(VIDEO_PATH_INDEX[gid_str])
        else:
            missing.append(gid_str)

    if missing:
        print(f"âš ï¸  ë§¤í•‘ ëˆ„ë½ (íŒŒì¼ ì—†ìŒ) gloss_id: {missing}")
    return paths
# ======================================================================
# ì˜ìƒ í•©ì„±/ì €ì¥ (ì›í•˜ë©´ service.pyì—ì„œ ì‚¬ìš© ê°€ëŠ¥)
# ======================================================================

def get_korean_font(size=80):
    font_paths = [
        "C:/Windows/Fonts/malgun.ttf",
        "/System/Library/Fonts/AppleSDGothicNeo.ttc",
        "/usr/share/fonts/truetype/nanum/NanumGothic.ttf",
        "AppleGothic.ttf",
    ]
    for path in font_paths:
        try:
            return ImageFont.truetype(path, size)
        except Exception:
            continue
    return ImageFont.load_default()


def generate_image_video(text: str, duration: float = 2.0) -> str:
    with tempfile.NamedTemporaryFile(suffix=".png", delete=False) as tf:
        img_path = tf.name

    width, height = 1280, 720
    img = Image.new("RGB", (width, height), color="black")
    d = ImageDraw.Draw(img)

    font = get_korean_font(80)

    bbox = d.textbbox((0, 0), text, font=font)
    text_w = bbox[2] - bbox[0]
    text_h = bbox[3] - bbox[1]
    position = ((width - text_w) / 2, (height - text_h) / 2)

    d.text(position, text, font=font, fill="white")
    img.save(img_path)

    out_mp4 = img_path.replace(".png", ".mp4")
    cmd = [
        "ffmpeg", "-y",
        "-loop", "1", "-i", img_path,
        "-t", str(duration),
        "-c:v", "libx264",
        "-preset", "veryfast",
        "-profile:v", "high",
        "-pix_fmt", "yuv420p",
        "-r", "30",
        "-video_track_timescale", "90000",
        "-bf", "2",
        "-an",
        "-vf", "scale=1280:720",
        "-loglevel", "error",
        out_mp4,
    ]
    subprocess.run(cmd, check=True)

    try:
        os.remove(img_path)
    except Exception:
        pass

    return out_mp4


def generate_blank_video(duration: float = 1.0) -> str:
    with tempfile.NamedTemporaryFile(suffix=".mp4", delete=False) as tf:
        out_mp4 = tf.name

    cmd = [
        "ffmpeg", "-y",
        "-f", "lavfi", "-i", f"color=c=black:s=1280x720:d={duration}",
        "-c:v", "libx264",
        "-preset", "veryfast",
        "-profile:v", "high",
        "-pix_fmt", "yuv420p",
        "-r", "30",
        "-video_track_timescale", "90000",
        "-bf", "2",
        "-an",
        "-loglevel", "error",
        out_mp4,
    ]
    subprocess.run(cmd, check=True)
    return out_mp4


def play_sequence(paths):
    if not paths:
        print("âš ï¸ ì¬ìƒí•  ì˜ìƒì´ ì—†ìŠµë‹ˆë‹¤.")
        return False

    ffmpeg = shutil.which("ffmpeg") or "ffmpeg"
    ffplay = shutil.which("ffplay")

    if ffplay:
        with tempfile.NamedTemporaryFile("w", suffix=".txt", delete=False, encoding="utf-8") as f:
            lst_path = f.name
            for p in paths:
                safe_path = str(Path(p).resolve()).replace("\\", "/")
                f.write(f"file '{safe_path}'\n")
        try:
            cmd = [
                ffplay,
                "-autoexit",
                "-hide_banner",
                "-loglevel", "error",
                "-f", "concat",
                "-safe", "0",
                "-i", lst_path,
            ]
            subprocess.run(cmd, check=True)
            return True
        finally:
            try:
                os.remove(lst_path)
            except OSError:
                pass

    with tempfile.TemporaryDirectory() as td:
        lst = Path(td) / "list.txt"
        out = Path(td) / f"concat_{now_ts()}.mp4"

        with open(lst, "w", encoding="utf-8") as f:
            for p in paths:
                safe_path = str(Path(p).resolve()).replace("\\", "/")
                f.write(f"file '{safe_path}'\n")

        copy_cmd = [
            ffmpeg, "-y",
            "-f", "concat", "-safe", "0",
            "-i", str(lst),
            "-c", "copy",
            str(out),
        ]
        r = subprocess.run(copy_cmd)

        if r.returncode != 0:
            re_cmd = [
                ffmpeg, "-y",
                "-f", "concat", "-safe", "0",
                "-i", str(lst),
                "-vf", "format=yuv420p",
                "-c:v", "libx264", "-crf", "20", "-preset", "veryfast",
                "-c:a", "aac", "-b:a", "128k",
                str(out),
            ]
            subprocess.run(re_cmd, check=True)

        if sys.platform == "darwin":
            subprocess.Popen(["open", str(out)])
        elif os.name == "nt":
            os.startfile(str(out))
        else:
            subprocess.Popen(["xdg-open", str(out)])
        return True


def save_sequence(paths, output_path: Path):
    if not paths:
        return

    ffmpeg = shutil.which("ffmpeg") or "ffmpeg"

    with tempfile.NamedTemporaryFile("w", suffix=".txt", delete=False, encoding="utf-8") as f:
        lst_path = f.name
        for p in paths:
            safe_path = str(Path(p).resolve()).replace("\\", "/")
            f.write(f"file '{safe_path}'\n")

    try:
        cmd = [
            ffmpeg, "-y",
            "-f", "concat", "-safe", "0",
            "-i", lst_path,
            "-c:v", "libx264", "-pix_fmt", "yuv420p",
            "-c:a", "aac",
            "-loglevel", "error",
            str(output_path),
        ]
        subprocess.run(cmd, check=True)
        print(f"ğŸ’¾ ì˜ìƒ ì €ì¥ ì™„ë£Œ: {output_path}")
    except Exception as e:
        print(f"âŒ ì˜ìƒ ì €ì¥ ì‹¤íŒ¨: {e}")
    finally:
        try:
            os.remove(lst_path)
        except Exception:
            pass


# ëª¨ë“ˆ ë¡œë“œ ì‹œ Gemini ëª¨ë¸ í•œ ë²ˆë§Œ ë¹Œë“œ
if GOOGLE_API_KEY and genai is not None:
    try:
        GEMINI_MODEL = build_gemini()
        print("[Gemini] ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")
    except Exception as e:
        GEMINI_MODEL = None
        print(f"[Gemini] ì´ˆê¸°í™” ì‹¤íŒ¨, ë¡œì»¬ ê·œì¹™ë§Œ ì‚¬ìš©: {e}")
else:
    GEMINI_MODEL = None
    print("[Gemini] API í‚¤ ì—†ìŒ â†’ ë¡œì»¬ ê·œì¹™ë§Œ ì‚¬ìš©")

# ğŸ”¹ Whisper ëª¨ë¸ë„ ì„œë²„ ì‹œì‘ ì‹œ ë¯¸ë¦¬ ë¡œë”©
try:
    _get_whisper_model()
    print("[Whisper] ëª¨ë¸ ë¯¸ë¦¬ ë¡œë”© ì™„ë£Œ")
except Exception as e:
    print(f"[Whisper] ëª¨ë¸ ë¯¸ë¦¬ ë¡œë”© ì‹¤íŒ¨: {e}")


# ======================================================================
# ë¡œì»¬ ê·œì¹™ ê¸°ë°˜ gloss ì¶”ì¶œ (service.pyì—ì„œ Gemini ì‹¤íŒ¨ ì‹œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ìµœì†Œ ë²„ì „)
# ======================================================================

def _local_gloss_rules(text: str) -> list[str]:
    """
    Gemini ì—†ì´ë„ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ì´ˆê°„ë‹¨ í´ë°± ê·œì¹™.
    (ê¸°ì¡´ extract_glossesì˜ ë¡œì»¬ í´ë°±ê³¼ ë™ì¼í•œ ìˆ˜ì¤€)
    """
    clean = _norm(text)
    tokens = re.findall(r"\d+(?:ì–µì›|ì–µ\s*ì›|ê°œì›”|ë…„|ì„¸|%)|[ê°€-í£A-Za-z]+", clean)
    return [_first_word(t) for t in tokens if _first_word(t)]
