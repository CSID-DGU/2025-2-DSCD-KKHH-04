# -*- coding: utf-8 -*-
# new

"""
Django ë°±ì—”ë“œìš© íŒŒì´í”„ë¼ì¸ ëª¨ë“ˆ

ì—­í• :
- WAV/WEBM ë“± ìŒì„± íŒŒì¼ -> STT(whisper)
- STT í…ìŠ¤íŠ¸ -> Gemini ê¸°ë°˜ í† í° ì¶”ì¶œ(tokens: gloss/image/pause)
- ê°„ë‹¨ ë²„ì „: extract_glosses(text, model) -> gloss ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸ (ê¸°ì¡´ service.py í˜¸í™˜)
- gloss ë¦¬ìŠ¤íŠ¸ -> gloss_id ë§¤í•‘(CSV ì‚¬ì „)
- gloss_id -> ìˆ˜ì–´ ì˜ìƒ(mp4) ê²½ë¡œ ë¦¬ìŠ¤íŠ¸

service.pyì—ì„œ import í•˜ëŠ” ì‹¬ë³¼:
    stt_from_file
    extract_glosses
    to_gloss_ids
    load_gloss_index
    _paths_from_ids
    build_gemini
    MEDIA_ROOT
    now_ts
    OUT_DIR
    _norm
    GEMINI_MODEL
    _local_gloss_rules
"""

import os
import csv
import re
import json
import ast
import unicodedata
import difflib
import wave
import sys
from datetime import datetime
from pathlib import Path
import shutil
import subprocess
import tempfile
import time  # ë””ë²„ê¹…ìš©

import whisper
from dotenv import load_dotenv

from PIL import Image, ImageDraw, ImageFont

# Gemini ë¼ì´ë¸ŒëŸ¬ë¦¬
try:
    import google.generativeai as genai
except Exception:
    genai = None

# Django MEDIA_ROOT ì—°ë™ (ì—†ìœ¼ë©´ ë¡œì»¬ media í´ë” ì‚¬ìš©)
try:
    from django.conf import settings

    MEDIA_ROOT = Path(getattr(settings, "MEDIA_ROOT", "media")).resolve()
except Exception:
    MEDIA_ROOT = Path(__file__).resolve().parent / "media"

# 1. .env ë¡œë“œ
load_dotenv()

# 2. í™˜ê²½ ë³€ìˆ˜
print("[PIPELINE] GOOGLE_API_KEY =", os.getenv("GOOGLE_API_KEY"))
GOOGLE_API_KEY = (os.getenv("GOOGLE_API_KEY") or "").strip().strip("'\"")
print("[DEBUG] GOOGLE_API_KEY prefix:", (GOOGLE_API_KEY or "")[:15], "...")

if not GOOGLE_API_KEY:
    print("âš ï¸  [Warn] GOOGLE_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. Gemini ì—†ì´ ë¡œì»¬ ê·œì¹™ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.")

# 3. ê²½ë¡œ ì„¤ì •
ROOT_DIR = Path(__file__).resolve().parent

GLOSS_NEW_DIR = ROOT_DIR / "gloss_new"

DATA_DIR = GLOSS_NEW_DIR / "data"  # backend/pipelines/gloss_new/data
OUT_DIR = GLOSS_NEW_DIR / "snapshots14"

# ì´ ê²½ë¡œë“¤ì€ ë„¤ í”„ë¡œì íŠ¸ êµ¬ì¡°ì— ë§ê²Œ í•œ ë²ˆ í™•ì¸í•´ì¤˜
GLOSS_DICT_PATH = DATA_DIR / "gloss_dictionary_MOCK.csv"

# ê·œì¹™ íŒŒì¼ ë‘ ê°œ ì‚¬ìš©:
# - rules_base.json: ì‚¬ëŒì´ ê´€ë¦¬í•˜ëŠ” ê¸°ë³¸ ê·œì¹™
# - rules.json: í•™ìŠµ/ì¶”ê°€ ê·œì¹™ í¬í•¨ ì‹¤ì œ ìš´ì˜ ê·œì¹™
RULES_PATH = DATA_DIR / "rules.json"  # ì‹¤ì œ ì‚¬ìš© Â· ìë™ ì—…ë°ì´íŠ¸ ëŒ€ìƒ
RULES_BASE_PATH = DATA_DIR / "rules_base.json"

GLOSS_MP4_DIR = Path(
    r"C:\Users\user\Desktop\2025-2-DSCD-KKHH-04-git\backend\pipelines\gloss_new\data\service"
)
# ìˆ˜ì–´ mp4ê°€ ìˆëŠ” ë£¨íŠ¸ í´ë” (í•˜ìœ„ fi, li ë“± í¬í•¨)

VIDEO_OUT_DIR = GLOSS_NEW_DIR / "vd_output"
OUT_DIR.mkdir(exist_ok=True)
VIDEO_OUT_DIR.mkdir(exist_ok=True)

# ğŸ”¹ gloss ë§¤í•‘ ë¡œê·¸ ì €ì¥ í´ë”/íŒŒì¼ (ì›í•˜ë©´ ë‚˜ì¤‘ì— service.pyì—ì„œ ì‚¬ìš©)
LOG_DIR = ROOT_DIR / "gloss_tools"
LOG_DIR.mkdir(parents=True, exist_ok=True)
GLOSS_LOG_FILE = LOG_DIR / "gloss_mapping_log.csv"


# =========================
# rules_base.json + rules.json ìœ í‹¸
# =========================
def _load_json(path: Path) -> dict:
    """
    JSON íŒŒì¼ì„ ì•ˆì „í•˜ê²Œ ì½ì–´ì„œ dictë¡œ ë°˜í™˜.
    íŒŒì¼ì´ ì—†ê±°ë‚˜ í˜•ì‹ì´ ì˜ëª»ë˜ë©´ ë¹ˆ dict ë°˜í™˜.
    """
    if not path.exists():
        return {}
    try:
        with path.open("r", encoding="utf-8") as f:
            return json.load(f)
    except Exception:
        return {}


def _save_json(path: Path, data: dict):
    """
    dictë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥.
    ìƒìœ„ ë””ë ‰í„°ë¦¬ê°€ ì—†ìœ¼ë©´ ìƒì„±í•œë‹¤.
    """
    path.parent.mkdir(parents=True, exist_ok=True)
    with path.open("w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)


def merge_rules() -> dict:
    """
    rules_base.json + rules.jsonì„ í•©ì³ì„œ í•˜ë‚˜ì˜ dictë¡œ ë°˜í™˜.

    êµ¬ì¡° ì˜ˆ:
    {
      "disambiguation_rules": { ... },
      "text_normalization": [ {...}, {...} ]
    }

    - disambiguation_rules: learned(rules.json)ì´ baseë¥¼ ë®ì–´ì”€
    - text_normalization: base + learned ìˆœì„œëŒ€ë¡œ ì´ì–´ ë¶™ì„
    """
    base = _load_json(RULES_BASE_PATH)
    learned = _load_json(RULES_PATH)

    base_dis = base.get("disambiguation_rules", {}) or {}
    learned_dis = learned.get("disambiguation_rules", {}) or {}

    base_norm = base.get("text_normalization", []) or []
    learned_norm = learned.get("text_normalization", []) or []

    return {
        "disambiguation_rules": {
            **base_dis,
            **learned_dis,  # learnedê°€ ìˆìœ¼ë©´ baseë¥¼ ë®ì–´ì”€
        },
        "text_normalization": base_norm + learned_norm,
    }


def append_learned_rule(wrong: str, correct: str):
    """
    wrong â†’ correct ê·œì¹™ì„ rules.json(text_normalization)ì— ì¶”ê°€.
    rules_base.jsonì€ ê±´ë“œë¦¬ì§€ ì•ŠëŠ”ë‹¤.
    """
    wrong = (wrong or "").strip()
    correct = (correct or "").strip()
    if not wrong or not correct:
        return

    data = _load_json(RULES_PATH)
    if not isinstance(data, dict):
        data = {}

    tn_list = data.get("text_normalization", [])
    if not isinstance(tn_list, list):
        tn_list = []

    # ì¤‘ë³µ ë°©ì§€
    for r in tn_list:
        if r.get("wrong") == wrong and r.get("correct") == correct:
            return  # ì´ë¯¸ ë™ì¼ ê·œì¹™ ì¡´ì¬

    tn_list.append({"wrong": wrong, "correct": correct})
    data["text_normalization"] = tn_list
    _save_json(RULES_PATH, data)


# ëª¨ë“ˆ ë¡œë“œ ì‹œ base+learned ê·œì¹™ í•œ ë²ˆ ë¨¸ì§€í•´ì„œ ì „ì—­ìœ¼ë¡œ ë³´ê´€
MERGED_RULES = merge_rules()


def append_normalization_rule(wrong: str, correct: str):
    """
    Django views(add_rule)ì—ì„œ ì‚¬ìš©í•˜ëŠ” wrapper.

    - rules.json(text_normalization)ì— ê·œì¹™ ì¶”ê°€
    - MERGED_RULESë„ ë‹¤ì‹œ ë¨¸ì§€í•´ì„œ ìµœì‹  ìƒíƒœë¡œ ê°±ì‹ 
    """
    global MERGED_RULES
    append_learned_rule(wrong, correct)
    MERGED_RULES = merge_rules()


def apply_text_normalization(text: str, rules: dict | None = None) -> str:
    """
    rules['text_normalization']ì— ìˆëŠ”
    {wrong, correct} ë¦¬ìŠ¤íŠ¸ë¥¼ ìˆœì„œëŒ€ë¡œ ì ìš©í•´ì„œ í…ìŠ¤íŠ¸ ì •ê·œí™”.
    """
    if not text:
        return text

    # âœ… rules íŒŒë¼ë¯¸í„°ê°€ ì•ˆ ë“¤ì–´ì˜¤ë©´, ë§¤ë²ˆ ìµœì‹  rules_base.json + rules.jsonì„ ë‹¤ì‹œ í•©ì³ì„œ ì‚¬ìš©
    if rules is None:
        rules = merge_rules()

    norm_rules = rules.get("text_normalization", []) or []
    out = text
    for r in norm_rules:
        w = (r.get("wrong") or "").strip()
        c = (r.get("correct") or "").strip()
        if not w or not c:
            continue
        out = out.replace(w, c)
    return out



def log_gloss_mapping(
    gloss_list,
    gloss_ids,
    gloss_labels,
    text=None,
    mode=None,
    session_id=None,
    ts=None,
    only_mismatch=True,
):
    """
    gloss / gloss_ids / gloss_labels ë§¤í•‘ ê²°ê³¼ë¥¼ CSVë¡œ ê¸°ë¡.
    """
    if gloss_list is None:
        gloss_list = []
    if gloss_ids is None:
        gloss_ids = []
    if gloss_labels is None:
        gloss_labels = []

    has_mismatch = any((g != l) for g, l in zip(gloss_list, gloss_labels))

    if only_mismatch and not has_mismatch:
        return

    if ts is None:
        ts = datetime.now().strftime("%Y%m%d_%H%M%S")

    row = {
        "timestamp": ts,
        "session_id": session_id or "",
        "mode": mode or "",
        "text": text or "",
        "gloss": "|".join(gloss_list),
        "gloss_ids": "|".join(gloss_ids),
        "gloss_labels": "|".join(gloss_labels),
        "has_mismatch": "1" if has_mismatch else "0",
    }

    file_exists = GLOSS_LOG_FILE.exists()
    with GLOSS_LOG_FILE.open("a", newline="", encoding="utf-8") as f:
        writer = csv.DictWriter(
            f,
            fieldnames=[
                "timestamp",
                "session_id",
                "mode",
                "text",
                "gloss",
                "gloss_ids",
                "gloss_labels",
                "has_mismatch",
            ],
        )
        if not file_exists:
            writer.writeheader()
        writer.writerow(row)

# íŒŒì¼ ì—…ë¡œë“œ
print("ğŸ”„ NEW pipeline.py loaded")
print("ğŸ“ GLOSS_DICT_PATH   =", GLOSS_DICT_PATH)
print("ğŸ“ RULES_BASE_PATH   =", RULES_BASE_PATH)
print("ğŸ“ RULES_PATH        =", RULES_PATH)
print("ğŸ“ GLOSS_MP4_DIR     =", GLOSS_MP4_DIR)
print("ğŸ“ GLOSS_LOG_FILE    =", GLOSS_LOG_FILE)

# 4. ëª¨ë¸/ì˜¤ë””ì˜¤ ì„¤ì •
GEMINI_MODEL_NAME = "models/gemini-2.5-flash"
WHISPER_MODEL_NAME = "small"
WHISPER_LANG = "ko"

ALWAYS_RETURN_ID = True  # ë§¤í•‘ ì‹¤íŒ¨ ì‹œì—ë„ ìœ ì‚¬ë„ ê¸°ë°˜ìœ¼ë¡œ ID í•˜ë‚˜ëŠ” ì„ íƒ

# ì „ì—­ ìºì‹œ
GEMINI_MODEL = None
_WHISPER_MODEL = None
WHISPER_LOAD_MS = None

# ======================================================================
# ê³µí†µ ìœ í‹¸
# ======================================================================
def _norm(s: str) -> str:
    """ì „ê°/ë°˜ê° í†µì¼ + ì–‘ ë ê³µë°± ì œê±° + ë‚´ë¶€ ë‹¤ì¤‘ ê³µë°±ì„ 1ì¹¸ìœ¼ë¡œ ì¶•ì†Œ."""
    s = unicodedata.normalize("NFKC", s or "").strip()
    s = re.sub(r"\s+", " ", s)
    return s


def _first_word(phrase: str) -> str:
    """ë¬¸ì¥ì—ì„œ ì²« ë‹¨ì–´ë§Œ ì¶”ì¶œ(ê¸€ë¡œìŠ¤ëŠ” ë‹¨ì–´ 1ê°œë¼ëŠ” ì „ì œ ìœ ì§€ìš©)."""
    s = _norm(phrase)
    return s.split()[0] if s else ""


def _nospace(s: str) -> str:
    """ê³µë°±/ê¸°í˜¸ ì œê±° í›„ ë¹„êµìš© í‚¤ ìƒì„±."""
    return re.sub(r"[^\wê°€-í£]", "", re.sub(r"\s+", "", _norm(s)))


def now_ts() -> str:
    """í˜„ì¬ ì‹œê°ì„ YYYYmmdd_HHMMSS ë¬¸ìì—´ë¡œ ë°˜í™˜."""
    return datetime.now().strftime("%Y%m%d_%H%M%S")


# ======================================================================
# STT (íŒŒì¼ ê¸°ë°˜) - service.pyì—ì„œ ì‚¬ìš©
# ======================================================================
def _get_whisper_model():
    global _WHISPER_MODEL, WHISPER_LOAD_MS
    if _WHISPER_MODEL is None:
        print(f"[Whisper] loading model: {WHISPER_MODEL_NAME}")
        t0 = time.perf_counter()
        try:
            _WHISPER_MODEL = whisper.load_model(WHISPER_MODEL_NAME, device="cpu")
            WHISPER_LOAD_MS = (time.perf_counter() - t0) * 1000.0
            print(
                f"[Whisper Init] whisper.load_model('{WHISPER_MODEL_NAME}') "
                f"{WHISPER_LOAD_MS:.1f} ms"
            )
        except Exception as e:
            print(f"[Whisper] ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            raise
    return _WHISPER_MODEL


def stt_from_file(audio_path: str) -> str:
    """
    ì„œë²„ì—ì„œ íŒŒì¼ ê²½ë¡œë¥¼ ë°›ì•„ STT ìˆ˜í–‰ í›„ í…ìŠ¤íŠ¸ ë°˜í™˜.
    """
    model = _get_whisper_model()
    t0 = time.perf_counter()
    res = model.transcribe(
        str(audio_path),
        language=WHISPER_LANG,
        fp16=False,
        temperature=0.0,
        beam_size=1,
        best_of=1,
        condition_on_previous_text=False,
        no_speech_threshold=0.05,
        logprob_threshold=-2.0,
        compression_ratio_threshold=2.0,
    )
    t1 = time.perf_counter()
    print(f"[STT inner] whisper.transcribe only: {t1 - t0:.2f} sec for {audio_path}")

    stt_text = _norm(res.get("text") or "")
    print(f"[STT] {audio_path} -> \"{stt_text}\"")
    return stt_text


# ======================================================================
# Gemini ì„¤ì • ë° í† í° ì¶”ì¶œ (ê³ ê¸‰ ë²„ì „)
# ======================================================================
def build_gemini():
    """
    Gemini ëª¨ë¸ ìƒì„±.
    """
    if not GOOGLE_API_KEY or genai is None:
        return None

    genai.configure(api_key=GOOGLE_API_KEY)

    sys_prompt = f"""

    ë‹¹ì‹ ì€ 'ì²­ê°ì¥ì• ì¸ì„ ìœ„í•œ ì „ë¬¸ ìˆ˜ì–´(KSL) í†µì—­ì‚¬'ì…ë‹ˆë‹¤.

    ì…ë ¥ëœ ë¬¸ì¥ì„ ë‹¨ìˆœ ë²ˆì—­í•˜ì§€ ë§ê³ , 'ë†ë¬¸í™”(Deaf Culture)'ì™€ 'í•œêµ­ìˆ˜ì–´ ë¬¸ë²•'ì— ë§ì¶° ì˜ë¯¸ë¥¼ ì¬êµ¬ì„±(Paraphrasing)í•˜ì‹­ì‹œì˜¤.



    [í•µì‹¬ ì‘ì—… ì›ì¹™]

    1. ìˆ˜ì§€í•œêµ­ì–´(SK) ê¸ˆì§€: í•œêµ­ì–´ì˜ ì–´ìˆœì´ë‚˜ ë¬¸ë²• ìš”ì†Œ(ì¡°ì‚¬, ì–´ë¯¸)ë¥¼ ê·¸ëŒ€ë¡œ ë”°ë¼ê°€ì§€ ë§ˆì‹­ì‹œì˜¤.

    2. ì˜ë¯¸ ì¤‘ì‹¬ ë²ˆì—­: ë¬¸ì¥ì˜ 'í•µì‹¬ ì˜ë„'ë¥¼ íŒŒì•…í•˜ì—¬ ê°€ì¥ ì§ê´€ì ì¸ ë‹¨ì–´ë“¤ì˜ ë‚˜ì—´ë¡œ ë°”ê¾¸ì‹­ì‹œì˜¤.

    3. ë©”íƒ€ ë°œí™” ì‚­ì œ: "ì•ˆë‚´í•´ ë“œë¦¬ê² ìŠµë‹ˆë‹¤", "ë§ì”€ë“œë¦¬ìë©´" ë“± ì •ë³´ê°€ê°€ ì—†ëŠ” ë©˜íŠ¸ëŠ” ê³¼ê°íˆ ì‚­ì œí•˜ì‹­ì‹œì˜¤.

       - ë‹¨, 'ì•ˆë…•í•˜ì„¸ìš”', 'ë°˜ê°‘ìŠµë‹ˆë‹¤', 'ê³ ë§™ìŠµë‹ˆë‹¤(ê°ì‚¬í•©ë‹ˆë‹¤)', 'ìˆ˜ê³ í•˜ì…¨ìŠµë‹ˆë‹¤' ë“± ì‚¬íšŒì  ê´€ê³„ë¥¼ ë§ºëŠ” ì¸ì‚¬ë§ì€ ì‚­ì œí•˜ì§€ ë§ê³  ë°˜ë“œì‹œ ìˆ˜ì–´ ë‹¨ì–´ë¡œ ë³€í™˜í•˜ì‹­ì‹œì˜¤.

    4. í•œêµ­ì–´ ì „ìš© ì¶œë ¥ (Korean Only):

       - ê²°ê³¼ JSONì˜ 'text' í•„ë“œ ê°’ì—ëŠ” 'ë°˜ë“œì‹œ í•œêµ­ì–´ ë˜ëŠ” ìˆ«ì'ë§Œ ë“¤ì–´ê°€ì•¼ í•©ë‹ˆë‹¤.

       - ì˜ì–´ ë‹¨ì–´(ì˜ˆ: 'Limit', 'Bank')ê°€ í¬í•¨ë˜ë©´ ë¬´ì¡°ê±´ í•œêµ­ì–´ ëœ»ìœ¼ë¡œ ë²ˆì—­í•˜ì—¬ ì¶œë ¥í•˜ì‹­ì‹œì˜¤.

        [cleaned í•„ë“œ ê·œì¹™ â€“ ì•„ì£¼ ì¤‘ìš”]
    1. cleanedëŠ” í™”ë©´ì— ìë§‰ìœ¼ë¡œ ê·¸ëŒ€ë¡œ í‘œì‹œë , ì‚¬ëŒìš© ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ ë¬¸ì¥ì…ë‹ˆë‹¤.
    2. ì…ë ¥ ë¬¸ì¥ì˜ ì¡´ëŒ“ë§/ì–´ë¯¸/ë†’ì„(ì˜ˆ: ì£¼ì„¸ìš”, ì…ë‹ˆë‹¤, í•´ìš”)ì„ ìœ ì§€í•˜ì‹­ì‹œì˜¤.
       - ë™ì‚¬Â·í˜•ìš©ì‚¬ë¥¼ ì‚¬ì „í˜•(ì£¼ë‹¤, í•˜ë‹¤, ê°€ë‹¤)ë¡œ ë°”ê¾¸ì§€ ë§ˆì‹­ì‹œì˜¤.
    3. ì˜ë¯¸ ì—†ëŠ” ì•ë’¤ ë©˜íŠ¸ë§Œ ì œê±°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
       - ì˜ˆ) ì…ë ¥: "ì§€ê¸ˆë¶€í„° ìƒí’ˆ ì„¤ëª… ë“œë¦¬ê² ìŠµë‹ˆë‹¤. ì‹ ë¶„ì¦ ì£¼ì„¸ìš”."
             cleaned: "ì‹ ë¶„ì¦ ì£¼ì„¸ìš”."
       - ì˜ˆ) ì…ë ¥: "ì‹ ë¶„ì¦ ì£¼ì„¸ìš”."
             cleaned: "ì‹ ë¶„ì¦ ì£¼ì„¸ìš”."
    4. cleanedëŠ” ë¬¸ì¥ë¶€í˜¸ë¥¼ ê°€ë³ê²Œ ë‹¤ë“¬ëŠ” ì •ë„ëŠ” í—ˆìš©ë˜ì§€ë§Œ, ë‹¨ì–´ í˜•íƒœëŠ” ìµœëŒ€í•œ ìœ ì§€í•˜ì‹­ì‹œì˜¤.

    [tokens í•„ë“œ ê·œì¹™]
    1. tokens ë°°ì—´ì€ ìˆ˜ì–´ í‘œí˜„ì„ ìœ„í•œ ë‹¨ìœ„ì…ë‹ˆë‹¤.
    2. tokensì˜ textëŠ” ìˆ˜ì–´ ë¬¸ë²•ì— ë§ê²Œ ë‹¨ì–´ ê¸°ë³¸í˜•ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
       - ì˜ˆ) cleaned: "ì‹ ë¶„ì¦ ì£¼ì„¸ìš”."
             tokens: [{{"text": "ì‹ ë¶„ì¦", "type": "gloss"}},
                      {{"text": "ì£¼ë‹¤", "type": "gloss"}}]
    3. "image" íƒ€ì…, "pause" íƒ€ì… ì‚¬ìš© ê·œì¹™ì€ ì•„ë˜ ë‚´ìš©ì„ ë”°ë¥´ì‹­ì‹œì˜¤.


    [ì¶œë ¥ í¬ë§·]
    {{
      "cleaned": "ì •ë¦¬ëœ í•œêµ­ì–´ ë¬¸ì¥",
      "tokens": [
         {{ "text": "ìƒí’ˆ", "type": "gloss" }},
         {{ "text": "1ëª…", "type": "image" }},
         {{ "text": "PAUSE", "type": "pause" }}
      ]
    }}
    ì´ JSONë§Œ ì¶œë ¥í•˜ì‹­ì‹œì˜¤.
    
    5. ê³ ìœ ëª…ì‚¬ ë° ìƒí’ˆëª… ì²˜ë¦¬ (Image Mapping):

       - ì‚¬ëŒì˜ ì´ë¦„(ì„±ëª…), ë‚¯ì„  ì§€ëª…, ë¸Œëœë“œëª…, ê·¸ë¦¬ê³  'êµ¬ì²´ì ì¸ ê¸ˆìœµ ìƒí’ˆëª…'ì€ ìˆ˜ì–´ë¡œ ì–µì§€ë¡œ ë²ˆì—­í•˜ê±°ë‚˜ ìª¼ê°œì§€ ë§ê³  ë°˜ë“œì‹œ ì „ì²´ë¥¼ í•˜ë‚˜ì˜ í…ìŠ¤íŠ¸ ì´ë¯¸ì§€ë¡œ ë³€í™˜í•˜ì‹­ì‹œì˜¤.

       - ì˜ì–´ì™€ í•œê¸€ì´ ì„ì—¬ ìˆì–´ë„ í•©ì³ì„œ í•˜ë‚˜ì˜ ì´ë¯¸ì§€ë¡œ ë§Œë“œì‹­ì‹œì˜¤.

       - ì˜ˆ: "ì €ëŠ” ê¹€ë™í˜¸ì…ë‹ˆë‹¤." -> '[ì €], [PAUSE], [ê¹€ë™í˜¸(image)]'

       - ì˜ˆ: "KBë‚˜ë¼ì‚¬ë‘ì ê¸ˆ ìƒí’ˆ" -> '[KBë‚˜ë¼ì‚¬ë‘ì ê¸ˆ(image)], [ìƒí’ˆ]'

    6. ì˜ë¬¸ë¬¸ ì²˜ë¦¬

       - ì§ˆë¬¸ì˜ ê²½ìš°ì—ëŠ” ë°˜ë“œì‹œ ëì— 'ë¬¼ìŒí‘œ'ë¡œ ë§ˆë¬´ë¦¬ í•˜ì‹­ì‹œì˜¤.

       - ì˜ˆ: "ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?" -> '[ë¬´ì—‡], [ë•ë‹¤], [?(image)]

       - ì˜ˆ: "ì§ì¥ì´ ìˆìœ¼ì‹ ê°€ìš”?" -> '[ì§ì¥], [ìˆë‹¤], [?(image)]'

       - ì˜ˆ: "í•„ìš”í•œ ê¸ˆì•¡ì€ ì–¼ë§ˆì¸ê°€ìš”?" -> '[í•„ìš”], [ê¸ˆì•¡], [ì–¼ë§ˆ], [?(image)]'

       - ì˜ˆ: "ì§„í–‰í• ê¹Œìš”?" -> '[ì§„í–‰], [?(image)]'

       

    [ë¬¸ë²• ë° êµ¬ì¡° ê·œì¹™ (Strict Rules)]

   

    1. í™”ì œ-ì„œìˆ  êµ¬ì¡° (Topic-Comment):

       - ë¬¸ì¥ ë§¨ ì•ì— [ì‹œê°„] -> [ì¥ì†Œ] -> [í™”ì œ(Topic)]ë¥¼ ë°°ì¹˜í•˜ì‹­ì‹œì˜¤.

       - í™”ì œì™€ ì„œìˆ ë¶€ ì‚¬ì´ì—ëŠ” ë°˜ë“œì‹œ `type: "pause"`ë¥¼ ì‚½ì…í•˜ì—¬ ì‹œê°ì  í˜¸í¡ì„ ì£¼ì‹­ì‹œì˜¤.

       - ì˜ˆ: "ì–´ì œ ì§‘ì—ì„œ ë°¥ì„ ë¨¹ì—ˆë‹¤" -> [ì–´ì œ], [ì§‘], [PAUSE], [ë°¥], [ë¨¹ë‹¤]

   

    2. ìˆ˜ëŸ‰ì‚¬ ë° ìˆ˜ì‹ì–´ í›„ì¹˜ (Post-position):

       - [ìˆ˜ëŸ‰]: 'í•œ ì‚¬ëŒ', 'ë‘ ê°œì˜ ê³„ì¢Œ'ëŠ” ë°˜ë“œì‹œ [ëª…ì‚¬] + [ìˆ˜ëŸ‰] ìˆœì„œë¡œ ë³€ê²½í•˜ì‹­ì‹œì˜¤.

         -> "í•œ ì‚¬ëŒ" (X) -> [ì‚¬ëŒ], [1ëª…(ì´ë¯¸ì§€)] (O)

       - [ë¶€ì •ì–´]: ì„œìˆ ì–´ ë’¤ì— ìœ„ì¹˜ì‹œí‚µë‹ˆë‹¤. (ì˜ˆ: [ê°€ë‹¤], [ì•ˆí•˜ë‹¤])

       - [í˜•ìš©ì‚¬]: ëª…ì‚¬ ë’¤ì— ìœ„ì¹˜ì‹œí‚µë‹ˆë‹¤. (ì˜ˆ: [ë”¸], [ì˜ˆì˜ë‹¤])



    3. ìˆ«ì ë° ë‹¨ìœ„ ì²˜ë¦¬ (ì´ë¯¸ì§€í™”):

       - ì˜¤ì¸ì‹ ë°©ì§€ë¥¼ ìœ„í•´ ìˆ«ìê°€ í¬í•¨ëœ ëª¨ë“  í‘œí˜„ì€ í…ìŠ¤íŠ¸ ì´ë¯¸ì§€ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

       - ê´€í˜•ì‚¬ 'í•œ, ë‘, ì„¸'ëŠ” ë°˜ë“œì‹œ ì•„ë¼ë¹„ì•„ ìˆ«ì '1, 2, 3'ìœ¼ë¡œ ë³€í™˜í•˜ì‹­ì‹œì˜¤.

       - % (í¼ì„¼íŠ¸): '[{{ "text": "3.5", "type": "image" }}, {{ "text": "í¼ì„¼íŠ¸", "type": "gloss" }}]'

       - %p (í¼ì„¼íŠ¸ í¬ì¸íŠ¸): '[{{ "text": "0.5", "type": "image" }}, {{ "text": "í¼ì„¼íŠ¸", "type": "gloss" }}, {{ "text": "í¬ì¸íŠ¸", "type": "gloss" }}]'

       - ì—° ì´ìœ¨: 'ì—°'ì€ `[1ë…„]` ìˆ˜ì–´ë¡œ, ì´ìœ¨ì€ '[í¼ì„¼íŠ¸]'ë¡œ ì²˜ë¦¬.



    4. ì–´íœ˜ ë‹¨ìˆœí™” (Vocabulary Simplification):

       - ì–´ë ¤ìš´ í•œìì–´, ì „ë¬¸ ìš©ì–´ëŠ” ê¸°ì´ˆì ì¸ ìˆ˜ì–´ ë‹¨ì–´ì˜ ì¡°í•©ìœ¼ë¡œ í’€ì–´ì„œ ì„¤ëª…í•˜ì‹­ì‹œì˜¤.

       - ì˜ˆ: "ì£¼íƒë‹´ë³´ëŒ€ì¶œ" -> '[ì§‘]', '[ë§¡ê¸°ë‹¤]', '[ëˆ]', '[ë¹Œë¦¬ë‹¤]'

       - ì˜ˆ: "ìš°ëŒ€ê¸ˆë¦¬" -> '[íŠ¹ë³„]', '[ì´ì]'



    [Few-shot Examples]



    ì…ë ¥: "ì´ ìƒí’ˆì€ í•œ ì‚¬ëŒë‹¹ í•˜ë‚˜ì˜ ê³„ì¢Œë§Œ ê°œì„¤ ê°€ëŠ¥í•©ë‹ˆë‹¤."

    ì¶œë ¥:

    {{

        "cleaned": "ìƒí’ˆ ì´ê²ƒ ì‚¬ëŒ 1ëª… ê³„ì¢Œ 1ê°œ ê°œì„¤ ê°€ëŠ¥",

        "tokens": [

            {{ "text": "ìƒí’ˆ", "type": "gloss" }},

            {{ "text": "ì´ê²ƒ", "type": "gloss" }},

            {{ "text": "PAUSE", "type": "pause" }},

            {{ "text": "ì‚¬ëŒ", "type": "gloss" }},

            {{ "text": "1ëª…", "type": "image" }},

            {{ "text": "ê³„ì¢Œ", "type": "gloss" }},

            {{ "text": "1ê°œ", "type": "image" }},
            {{ "text": "ê°œì„¤", "type": "gloss" }},
            {{ "text": "ê°€ëŠ¥", "type": "gloss" }}

        ]

    }}



    ì…ë ¥: "ê¸ˆë¦¬ëŠ” ì—° 3.5%í¬ì¸íŠ¸ ìš°ëŒ€ ì ìš©ë©ë‹ˆë‹¤."

    ì¶œë ¥:

    {{
        ]

        "cleaned": "ê¸ˆë¦¬ 1ë…„ 3.5 í¼ì„¼íŠ¸ ì ìˆ˜ íŠ¹ë³„ ì ìš©",

        "tokens": [

            {{ "text": "ê¸ˆë¦¬", "type": "gloss" }},
            {{ "text": "PAUSE", "type": "pause" }},
            {{ "text": "1ë…„", "type": "gloss" }},
            {{ "text": "3.5", "type": "image" }},
            {{ "text": "í¼ì„¼íŠ¸", "type": "gloss" }},
            {{ "text": "ì ìˆ˜", "type": "gloss" }},
            {{ "text": "íŠ¹ë³„", "type": "gloss" }},
            {{ "text": "ì ìš©", "type": "gloss" }}


    }}

    5. ë²”ìœ„ í‘œí˜„ (Range):
       - 'ì´ìƒ/ì´í•˜/ì´ˆê³¼/ë¯¸ë§Œ'ì€ ì˜¤ì—­ ë°©ì§€ë¥¼ ìœ„í•´ ë°˜ë“œì‹œ 'ë¶€í„°(~ë¶€í„°)'ì™€ 'ê¹Œì§€(~ê¹Œì§€)'ë¡œ ë³€í™˜í•˜ì‹­ì‹œì˜¤.
       - ì…ë ¥: "2.5% ì´ìƒ" -> '[{{ "text": "2.5", "type": "image" }}, {{ "text": "í¼ì„¼íŠ¸", "type": "gloss" }}, {{ "text": "ë¶€í„°", "type": "gloss" }}]'
       - ì…ë ¥: "3.5% ì´í•˜" -> '[{{ "text": "3.5", "type": "image" }}, {{ "text": "í¼ì„¼íŠ¸", "type": "gloss" }}, {{ "text": "ê¹Œì§€", "type": "gloss" }}]'
       - ì…ë ¥: "18ì„¸~30ì„¸" -> '[{{ "text": "18ì„¸", "type": "image" }}, {{ "text": "ë¶€í„°", "type": "gloss" }}, {{ "text": "30ì„¸", "type": "image" }}, {{ "text": "ê¹Œì§€", "type": "gloss" }}]'

       

    [ì¶œë ¥ í¬ë§· (JSON Only)]

    ë°˜ë“œì‹œ JSON í˜•ì‹ë§Œ ì¶œë ¥í•˜ì„¸ìš”.

    """
    


    model = genai.GenerativeModel(
        GEMINI_MODEL_NAME,
        system_instruction=sys_prompt,
        generation_config={
            "response_mime_type": "application/json",
            "temperature": 0.2,
        },
    )
    return model


def _get_gemini_model():
    global GEMINI_MODEL
    if GEMINI_MODEL is None:
        GEMINI_MODEL = build_gemini()
    return GEMINI_MODEL



def extract_tokens(text: str, model=None) -> list[dict]:
    """
    STT í…ìŠ¤íŠ¸ -> Gemini í† í°(JSON) -> tokens ë¦¬ìŠ¤íŠ¸ ì‚¬ìš©.
    - Geminiê°€ ì •ìƒ ì‘ë‹µí•˜ë©´: JSONì˜ tokens ê·¸ëŒ€ë¡œ ì‚¬ìš©
    - ì—ëŸ¬ / ì´ìƒ ì‘ë‹µì´ë©´: ë¡œì»¬ regex í´ë°± ì‚¬ìš©
    """
    clean = _norm(text)
    if not clean:
        return []
    
    # âœ… ì—¬ê¸°ì—ì„œ ê·œì¹™ ì ìš©
    clean = apply_text_normalization(clean)


    if model is None:
        model = _get_gemini_model()

    # 1) Gemini ì‚¬ìš©
    if model:
        try:
            print(f"[Gemini] call start  text={clean!r}")
            parts = [{"role": "user", "parts": [clean]}]
            t0 = time.perf_counter()
            resp = model.generate_content(parts)
            t1 = time.perf_counter()
            print(f"[Gemini] call done  {t1 - t0:.2f} sec")

            # --- Gemini ì‘ë‹µ(JSON) íŒŒì‹± ---
            raw = ""
            if getattr(resp, "text", None):
                raw = resp.text
            else:
                # candidatesì—ì„œ í…ìŠ¤íŠ¸ ëª¨ìœ¼ëŠ” ë°±ì—… ë¡œì§
                try:
                    cand = resp.candidates[0]
                    part_texts = []
                    for p in cand.content.parts:
                        if hasattr(p, "text"):
                            part_texts.append(p.text)
                    raw = "\n".join(part_texts)
                except Exception:
                    raw = ""

            raw = (raw or "").strip()
            if raw:
                # ```json ... ``` ë˜í•‘ ì œê±°
                if raw.startswith("```"):
                    raw = raw.strip("`")
                    if raw.lower().startswith("json"):
                        raw = raw[4:].lstrip()

                # ë³¸ë¬¸ì—ì„œ JSON ë¶€ë¶„ë§Œ ì˜ë¼ì„œ íŒŒì‹± (ìµœì´ˆ '{'ë¶€í„° ë§ˆì§€ë§‰ '}'ê¹Œì§€)
                start = raw.find("{")
                end = raw.rfind("}")
                if start != -1 and end != -1 and end > start:
                    raw_json = raw[start : end + 1]
                else:
                    raw_json = raw

                obj = json.loads(raw_json)

                tokens = obj.get("tokens") or []
                out: list[dict] = []
                for t in tokens:
                    if not isinstance(t, dict):
                        continue
                    txt = (t.get("text") or "").strip()
                    typ = (t.get("type") or "gloss").strip()
                    if not txt:
                        continue
                    out.append({"text": txt, "type": typ})

                if out:
                    print(f"[Gemini] tokens -> {out}")
                    return out

        except Exception as e:
            print(f"[Gemini Error] {e}")

    # 2) ë¡œì»¬ í´ë°±
    # 2-1) "ì €ëŠ” ê¹€ë‹¤ì˜ì…ë‹ˆë‹¤." / "ê¹€ë‹¤ì˜ì…ë‹ˆë‹¤." ê°™ì€ ì´ë¦„ íŒ¨í„´
    m = re.match(r"^(?:ì €ëŠ”\s*)?([ê°€-í£]{2,4})ì…ë‹ˆë‹¤[\.!]*$", clean)
    if m:
        name = m.group(1)
        print(f"[LocalFallback] ì´ë¦„ íŒ¨í„´ ê°ì§€ -> image í† í°ìœ¼ë¡œ ì‚¬ìš©: {name}")
        return [{"text": name, "type": "image"}]

    # 2-2) ê·¸ ì™¸ ì¼ë°˜ ê·œì¹™
    print(f"[LocalFallback] Gemini í† í° ì—†ìŒ â†’ regex ì‚¬ìš©  text={clean!r}")
    tokens = re.findall(
        r"""
        \d+(?:\.\d+)?(?:ì²œë§Œ|ì²œë§Œì›|ì²œì›|ì²œ)?(?:ì–µ|ì–µì›)?(?:ë§Œ|ë§Œì›)?(?:ì›)?
        |
        \d+(?:\.\d+)?(?:ê°œì›”|ë…„|ì„¸|%|í¼ì„¼íŠ¸|í¬ì¸íŠ¸)?
        |
        [ê°€-í£A-Za-z]+
        """,
        clean,
        re.VERBOSE,
    )
    return [{"text": _first_word(t), "type": "gloss"} for t in tokens if _first_word(t)]


def extract_glosses(text: str, model=None) -> list[str]:
    # 1) í† í° ë½‘ê¸°
    tokens = extract_tokens(text, model=model)

    # 2) ì‚¬ì „ ì¸ë±ìŠ¤ & ê·œì¹™ ë¶ˆëŸ¬ì˜¤ê¸°
    index = load_gloss_index()   # CSV ì‚¬ì „
    rules = MERGED_RULES

    gloss_words: list[str] = []

    for t in tokens:
        if not isinstance(t, dict):
            continue
        if t.get("type", "gloss") != "gloss":
            continue

        raw = (t.get("text") or "").strip()
        if not raw:
            continue

        # ì—¬ê¸°ì„œ resolve_gloss_token ì‚¬ìš©
        ids, resolve_logs = resolve_gloss_token(
            token_text=raw,
            original_sentence=text,
            rules=rules,
            db_index=index,
        )

        # id -> ì‹¤ì œ ì‚¬ì „ ë‹¨ì–´ë¡œ ë‹¤ì‹œ ë§¤í•‘
        id_to_word = index.get("id_to_word", {})
        for gid in ids:
            w = id_to_word.get(str(gid))
            if w:
                gloss_words.append(w)

    return gloss_words

    """
    service.py í˜¸í™˜ìš© ê°„ë‹¨ ì¸í„°í˜ì´ìŠ¤.
    """
    tokens = extract_tokens(text, model=model)
    gloss_list = [
        t["text"]
        for t in tokens
        if isinstance(t, dict) and t.get("type", "gloss") == "gloss" and t.get("text")
    ]
    return gloss_list


# ======================================================================
# Gloss ì‚¬ì „ ë¡œë“œ ë° ë§¤í•‘
# ======================================================================
VIDEO_PATH_INDEX = {}


def build_video_index(root_dir: Path):
    """
    í•˜ìœ„ í´ë” í¬í•¨ ëª¨ë“  mp4 íŒŒì¼ì„ ê²€ìƒ‰í•˜ì—¬
    { "íŒŒì¼ID": "ì „ì²´ê²½ë¡œ" } í˜•íƒœì˜ ì§€ë„ë¥¼ ë§Œë“¦.
    """
    global VIDEO_PATH_INDEX
    print(f"ğŸ“‚ ì˜ìƒ íŒŒì¼ ì¸ë±ì‹± ì¤‘... ({root_dir})")

    count = 0
    for path in root_dir.rglob("*.mp4"):
        file_id = path.stem
        VIDEO_PATH_INDEX[file_id] = str(path.resolve())
        count += 1

    print(f"âœ… ì´ {count}ê°œì˜ ì˜ìƒ íŒŒì¼ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.")


try:
    if GLOSS_MP4_DIR.exists():
        build_video_index(GLOSS_MP4_DIR)
    else:
        print(f"âš ï¸ GLOSS_MP4_DIRê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {GLOSS_MP4_DIR}")
except Exception as e:
    print(f"âš ï¸ build_video_index ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")


def load_gloss_index(csv_path: Path | str | None = None) -> dict:
    """
    ê¸€ë¡œìŠ¤ ì‚¬ì „ì„ ë¡œë“œí•´ ê²€ìƒ‰ìš© ì¸ë±ìŠ¤ë¥¼ ë§Œë“ ë‹¤.
    """
    if csv_path is None:
        csv_path = GLOSS_DICT_PATH

    csv_path = Path(csv_path)

    rows, exact = [], {}
    id_to_word = {}

    with open(csv_path, "r", encoding="utf-8-sig") as f:
        rdr = csv.DictReader(f)
        headers = [h.strip().lower() for h in (rdr.fieldnames or [])]

        def pick(*cands):
            for c in cands:
                if c in headers:
                    return c
            return None

        h_id = pick("gloss_id", "id", "gid")
        h_ko = pick(
            "korean_meanings",
            "korean",
            "ko",
            "meaning_ko",
            "ko_meanings",
            "korean_meaning",
        )
        h_cat1 = pick("cat_1", "category_1", "category")

        if not h_id or not h_ko:
            raise RuntimeError(f"[Gloss] í—¤ë” ê°ì§€ ì‹¤íŒ¨: {headers}")

        for row in rdr:
            gid = (row.get(h_id) or "").strip()
            cell = (row.get(h_ko) or "").strip()
            cat1 = (row.get(h_cat1) or "").strip() if h_cat1 else ""

            if not gid or not cell:
                continue

            try:
                obj = ast.literal_eval(cell)
                if isinstance(obj, (list, tuple)):
                    terms = [str(x) for x in obj]
                else:
                    terms = [str(obj)]
            except Exception:
                terms = [cell]

            if terms:
                id_to_word[gid] = terms[0]

            for term in terms:
                term = _norm(term)
                if not term:
                    continue
                term_ns = _nospace(term)
                token_cnt = len(term.split())
                char_len = len(term_ns)
                rows.append(
                    {
                        "gid": gid,
                        "term": term,
                        "term_ns": term_ns,
                        "token_cnt": token_cnt,
                        "char_len": char_len,
                        "cat_1": cat1,
                    }
                )
                exact.setdefault(term_ns, gid)

    if not rows:
        raise RuntimeError("[Gloss] ì‚¬ì „ì— ìœ íš¨í•œ í•­ëª©ì´ ì—†ìŠµë‹ˆë‹¤.")

    rows.sort(key=lambda x: 0 if "ì „ë¬¸ìš©ì–´" in (x.get("cat_1") or "") else 1)

    print(f"[Gloss] indexed rows={len(rows)}, exact_keys={len(exact)}")
    return {"rows": rows, "exact": exact, "id_to_word": id_to_word}


def map_one_word_to_id(word: str, index: dict, blacklist: list | None = None) -> str | None:
    if not word or not index:
        return None
    if blacklist is None:
        blacklist = []

    rows, exact = index["rows"], index["exact"]
    w = _first_word(word)
    wns = _nospace(w)
    if not wns:
        return None

    gid = exact.get(wns)
    if gid and int(gid) not in blacklist:
        return gid

    cands = [r for r in rows if wns in r["term_ns"] and int(r["gid"]) not in blacklist]
    if cands:
        cands.sort(key=lambda r: (r["token_cnt"], r["char_len"], r["term"], r["gid"]))
        return cands[0]["gid"]

    best_gid, best_sc = None, 0.0
    for r in rows:
        if int(r["gid"]) in blacklist:
            continue
        sc = difflib.SequenceMatcher(None, wns, r["term_ns"]).ratio()
        if sc > best_sc:
            best_sc, best_gid = sc, r["gid"]

    if ALWAYS_RETURN_ID and best_gid:
        return best_gid
    return None


def to_gloss_ids(gloss_list: list[str], index: dict) -> list[str]:
    """
    gloss_list: ["ìë™ì´ì²´", "ê°’", "gloss:ìë™ì´ì²´", "image:1ë…„", ...] ë“±
    """
    out: list[str] = []
    seen: set[str] = set()

    for raw in (gloss_list or []):
        if raw is None:
            continue

        g = str(raw).strip()
        if not g:
            continue

        if g.startswith("image:"):
            print(f"[to_gloss_ids] skip image token: {g!r}")
            continue

        if g.startswith("gloss:"):
            g_clean = g[len("gloss:") :].strip()
        else:
            g_clean = g

        if not g_clean:
            continue

        gid = map_one_word_to_id(g_clean, index)
        if not gid:
            print(f"[to_gloss_ids] no id for gloss='{g_clean}' (from {g!r})")
            continue

        gid_str = str(gid)
        if gid_str not in seen:
            out.append(gid_str)
            seen.add(gid_str)

    return out


def decompose_compound_word(token: str, valid_keys: dict) -> list[str] | None:
    if len(token) < 2:
        return None
    for i in range(1, len(token)):
        part1 = token[:i]
        part2 = token[i:]
        if part1 in valid_keys and part2 in valid_keys:
            return [part1, part2]
    return None


def resolve_gloss_token(token_text, original_sentence, rules, db_index):
    """
    ê³ ê¸‰ ê·œì¹™ ê¸°ë°˜ í† í° -> gloss_id ë§¤í•‘ í•¨ìˆ˜.
    """
    final_ids = []
    resolved_logs = []

    id_map = db_index.get("id_to_word", {})

    blacklist = rules.get("blacklist", [])
    sub_list = rules.get("word_substitution", {}).get(token_text, [token_text])

    for sub in sub_list:
        target_ids = []
        method = "unknown"

        if sub in rules.get("fixed_mappings", {}):
            target_ids.append(rules["fixed_mappings"][sub])
            method = "fixed_rule"

        elif sub in rules.get("disambiguation_rules", {}):
            rule = rules["disambiguation_rules"][sub]
            found = False
            for case in rule["cases"]:
                for kw in case["keywords"]:
                    if kw in original_sentence:
                        target_ids.append(case["target_id"])
                        found = True
                        method = f"context({kw})"
                        break
                if found:
                    break
            if not found:
                target_ids.append(rule["default_id"])
                method = "context_default"

        else:
            gid = map_one_word_to_id(sub, db_index, blacklist)
            if gid:
                target_ids.append(gid)
                method = "exact/similarity"
            else:
                decomposed = decompose_compound_word(sub, db_index["exact"])
                if decomposed:
                    for part in decomposed:
                        part_id = map_one_word_to_id(part, db_index, blacklist)
                        if part_id:
                            target_ids.append(part_id)
                    method = f"decomposed({decomposed})"

        if target_ids:
            final_ids.extend(target_ids)

            real_words = []
            for tid in target_ids:
                rw = id_map.get(str(tid), "UnknownID")
                real_words.append(rw)

            resolved_logs.append(
                {
                    "token": sub,
                    "resolved_word": real_words,
                    "ids": target_ids,
                    "method": method,
                }
            )

    return final_ids, resolved_logs


def _paths_from_ids(gloss_ids):
    """
    gloss_id ë¦¬ìŠ¤íŠ¸ë¥¼ ë°›ì•„ ë¯¸ë¦¬ ë§Œë“¤ì–´ë‘” ì§€ë„(VIDEO_PATH_INDEX)ì—ì„œ ê²½ë¡œë¥¼ ì°¾ìŒ.
    """
    paths, missing = [], []
    for gid in gloss_ids or []:
        gid_str = str(gid).strip()
        if not gid_str:
            continue

        if gid_str.startswith("image:"):
            print(f"[paths_from_ids] skip image token in gloss_ids: {gid_str!r}")
            continue
        if gid_str.startswith("gloss:"):
            print(f"[paths_from_ids] unexpected gloss: prefix in gloss_ids: {gid_str!r}")
            gid_str = gid_str[len("gloss:") :].strip()
            if not gid_str:
                continue

        if gid_str in VIDEO_PATH_INDEX:
            paths.append(VIDEO_PATH_INDEX[gid_str])
        else:
            missing.append(gid_str)

    if missing:
        print(f"âš ï¸  ë§¤í•‘ ëˆ„ë½ (íŒŒì¼ ì—†ìŒ) gloss_id: {missing}")
    return paths


# ======================================================================
# ì˜ìƒ í•©ì„±/ì €ì¥ ë° í…ìŠ¤íŠ¸ ì´ë¯¸ì§€ ì˜ìƒ (ìºì‹œ í¬í•¨)
# ======================================================================
IMAGE_VIDEO_CACHE: dict[str, str] = {}  # key: "text|duration" -> mp4 ê²½ë¡œ


def get_korean_font(size=80):
    font_paths = [
        "C:/Windows/Fonts/malgun.ttf",
        "/System/Library/Fonts/AppleSDGothicNeo.ttc",
        "/usr/share/fonts/truetype/nanum/NanumGothic.ttf",
        "AppleGothic.ttf",
    ]
    for path in font_paths:
        try:
            return ImageFont.truetype(path, size)
        except Exception:
            continue
    return ImageFont.load_default()


def generate_image_video(text: str, duration: float = 2.0) -> str:
    """
    ì‹¤ì œë¡œ ffmpegë¥¼ ëŒë ¤ í…ìŠ¤íŠ¸ ì´ë¯¸ì§€ ì˜ìƒì„ ìƒì„±.
    (ìºì‹± ì—†ì´ ìˆœìˆ˜ ìƒì„±ë§Œ ë‹´ë‹¹)
    """
    with tempfile.NamedTemporaryFile(suffix=".png", delete=False) as tf:
        img_path = tf.name

    width, height = 1280, 720
    img = Image.new("RGB", (width, height), color="black")
    d = ImageDraw.Draw(img)

    font = get_korean_font(80)

    bbox = d.textbbox((0, 0), text, font=font)
    text_w = bbox[2] - bbox[0]
    text_h = bbox[3] - bbox[1]
    position = ((width - text_w) / 2, (height - text_h) / 2)

    d.text(position, text, font=font, fill="white")
    img.save(img_path)

    out_mp4 = img_path.replace(".png", ".mp4")
    cmd = [
        "ffmpeg",
        "-y",
        "-loop",
        "1",
        "-i",
        img_path,
        "-t",
        str(duration),
        "-c:v",
        "libx264",
        "-preset",
        "veryfast",
        "-profile:v",
        "high",
        "-pix_fmt",
        "yuv420p",
        "-r",
        "30",
        "-video_track_timescale",
        "90000",
        "-bf",
        "2",
        "-an",
        "-vf",
        "scale=1280:720",
        "-loglevel",
        "error",
        out_mp4,
    ]
    subprocess.run(cmd, check=True)

    try:
        os.remove(img_path)
    except Exception:
        pass

    return out_mp4


def get_image_video_cached(text: str, duration: float = 2.0) -> str:
    """
    ê°™ì€ text + duration ì¡°í•©ì— ëŒ€í•´ í•œ ë²ˆë§Œ ffmpegë¥¼ ëŒë¦¬ê³ ,
    ì´í›„ì—ëŠ” ìºì‹œëœ mp4 ê²½ë¡œë¥¼ ì¬ì‚¬ìš©.
    """
    key = f"{text}|{duration}"
    mp4 = IMAGE_VIDEO_CACHE.get(key)

    # ìºì‹œì— ìˆê³  ì‹¤ì œ íŒŒì¼ë„ ì¡´ì¬í•˜ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
    if mp4 and os.path.exists(mp4):
        return mp4

    # ì—†ê±°ë‚˜ íŒŒì¼ì´ ì§€ì›Œì¡Œìœ¼ë©´ ìƒˆë¡œ ìƒì„±
    mp4 = generate_image_video(text, duration=duration)
    IMAGE_VIDEO_CACHE[key] = mp4
    return mp4


def generate_blank_video(duration: float = 1.0) -> str:
    with tempfile.NamedTemporaryFile(suffix=".mp4", delete=False) as tf:
        out_mp4 = tf.name

    cmd = [
        "ffmpeg",
        "-y",
        "-f",
        "lavfi",
        "-i",
        f"color=c=black:s=1280x720:d={duration}",
        "-c:v",
        "libx264",
        "-preset",
        "veryfast",
        "-profile:v",
        "high",
        "-pix_fmt",
        "yuv420p",
        "-r",
        "30",
        "-video_track_timescale",
        "90000",
        "-bf",
        "2",
        "-an",
        "-loglevel",
        "error",
        out_mp4,
    ]
    subprocess.run(cmd, check=True)
    return out_mp4

def build_video_sequence_from_tokens(
    tokens: list[dict],
    db_index: dict,
    original_text: str = "",
    rules: dict | None = None,
    include_pause: bool = False,
    pause_duration: float = 0.7,
    debug_log: bool = False,
) -> tuple[list[str], list[dict]]:
    """
    tokens ìˆœì„œë¥¼ ê·¸ëŒ€ë¡œ ë”°ë¼ê°€ë©´ì„œ
    - gloss  â†’ gloss_id â†’ ìˆ˜ì–´ mp4 ê²½ë¡œ
    - image  â†’ í…ìŠ¤íŠ¸ ì´ë¯¸ì§€ mp4 ê²½ë¡œ
    - pause  â†’ (ì˜µì…˜) ë¹ˆ í™”ë©´ mp4 ê²½ë¡œ
    ë¥¼ ì´ì–´ë¶™ì¸ video_listë¥¼ ë§Œë“ ë‹¤.

    ë°˜í™˜:
      video_paths: ì‹¤ì œ í•©ì„±ì— ì“¸ mp4 ê²½ë¡œ ë¦¬ìŠ¤íŠ¸ (ìˆœì„œ ë³´ì¥)
      debug_info : ê° í† í°ë³„ ë§¤í•‘ ê²°ê³¼(ê²€ì¦ìš©)
    """
    if not tokens:
        return [], []

    if rules is None:
        rules = MERGED_RULES

    video_paths: list[str] = []
    debug_info: list[dict] = []

    step_idx = 0

    for t in tokens:
        if not isinstance(t, dict):
            continue

        raw_text = (t.get("text") or "").strip()
        ttype = (t.get("type") or "gloss").strip().lower()
        if not raw_text:
            continue

        # 1) gloss í† í°: ê·œì¹™ + ì‚¬ì „ ê¸°ë°˜ìœ¼ë¡œ id â†’ mp4 ë§¤í•‘
        if ttype == "gloss":
            ids, resolve_logs = resolve_gloss_token(
                token_text=raw_text,
                original_sentence=original_text,
                rules=rules,
                db_index=db_index,
            )
            paths = _paths_from_ids(ids)

            video_paths.extend(paths)

            debug_entry = {
                "idx": step_idx,
                "token_type": "gloss",
                "token_text": raw_text,
                "ids": ids,
                "paths": paths,
                "resolve_logs": resolve_logs,
            }
            debug_info.append(debug_entry)

            if debug_log:
                print(
                    f"[SEQ][{step_idx:02d}] gloss '{raw_text}' "
                    f"-> ids={ids}, paths={paths}"
                )

            step_idx += 1

        # 2) image í† í°: í…ìŠ¤íŠ¸ ì´ë¯¸ì§€ ì˜ìƒ ìƒì„±(ë˜ëŠ” ìºì‹œ ì¬ì‚¬ìš©)
        elif ttype == "image":
            img_mp4 = get_image_video_cached(raw_text, duration=2.0)
            video_paths.append(img_mp4)

            debug_entry = {
                "idx": step_idx,
                "token_type": "image",
                "token_text": raw_text,
                "ids": [],
                "paths": [img_mp4],
                "resolve_logs": [],
            }
            debug_info.append(debug_entry)

            if debug_log:
                print(
                    f"[SEQ][{step_idx:02d}] image '{raw_text}' "
                    f"-> path={img_mp4}"
                )

            step_idx += 1

        # 3) pause í† í°: include_pause=Trueì¼ ë•Œë§Œ ë¹ˆ ì˜ìƒ ë¼ì›Œë„£ìŒ
        elif ttype == "pause":
            paths: list[str] = []
            if include_pause:
                blank_mp4 = generate_blank_video(duration=pause_duration)
                paths.append(blank_mp4)
                video_paths.append(blank_mp4)

                if debug_log:
                    print(
                        f"[SEQ][{step_idx:02d}] pause -> blank video "
                        f"duration={pause_duration}s, path={blank_mp4}"
                    )
            else:
                if debug_log:
                    print(
                        f"[SEQ][{step_idx:02d}] pause skipped "
                        f"(include_pause=False)"
                    )

            debug_entry = {
                "idx": step_idx,
                "token_type": "pause",
                "token_text": raw_text,
                "ids": [],
                "paths": paths,
                "resolve_logs": [],
            }
            debug_info.append(debug_entry)
            step_idx += 1

        # 4) ì•Œ ìˆ˜ ì—†ëŠ” íƒ€ì…ì€ ê·¸ëƒ¥ ë¬´ì‹œ
        else:
            if debug_log:
                print(
                    f"[SEQ][{step_idx:02d}] unknown type '{ttype}' "
                    f"for token '{raw_text}' -> skip"
                )
            debug_entry = {
                "idx": step_idx,
                "token_type": ttype,
                "token_text": raw_text,
                "ids": [],
                "paths": [],
                "resolve_logs": [],
            }
            debug_info.append(debug_entry)
            step_idx += 1

    return video_paths, debug_info


def play_sequence(paths):
    if not paths:
        print("âš ï¸ ì¬ìƒí•  ì˜ìƒì´ ì—†ìŠµë‹ˆë‹¤.")
        return False

    ffmpeg = shutil.which("ffmpeg") or "ffmpeg"
    ffplay = shutil.which("ffplay")

    if ffplay:
        with tempfile.NamedTemporaryFile(
            "w", suffix=".txt", delete=False, encoding="utf-8"
        ) as f:
            lst_path = f.name
            for p in paths:
                safe_path = str(Path(p).resolve()).replace("\\", "/")
                f.write(f"file '{safe_path}'\n")
        try:
            cmd = [
                ffplay,
                "-autoexit",
                "-hide_banner",
                "-loglevel",
                "error",
                "-f",
                "concat",
                "-safe",
                "0",
                "-i",
                lst_path,
            ]
            subprocess.run(cmd, check=True)
            return True
        finally:
            try:
                os.remove(lst_path)
            except OSError:
                pass

    with tempfile.TemporaryDirectory() as td:
        lst = Path(td) / "list.txt"
        out = Path(td) / f"concat_{now_ts()}.mp4"

        with open(lst, "w", encoding="utf-8") as f:
            for p in paths:
                safe_path = str(Path(p).resolve()).replace("\\", "/")
                f.write(f"file '{safe_path}'\n")

        copy_cmd = [
            ffmpeg,
            "-y",
            "-f",
            "concat",
            "-safe",
            "0",
            "-i",
            str(lst),
            "-c",
            "copy",
            str(out),
        ]
        r = subprocess.run(copy_cmd)

        if r.returncode != 0:
            re_cmd = [
                ffmpeg,
                "-y",
                "-f",
                "concat",
                "-safe",
                "0",
                "-i",
                str(lst),
                "-vf",
                "format=yuv420p",
                "-c:v",
                "libx264",
                "-crf",
                "20",
                "-preset",
                "veryfast",
                "-c:a",
                "aac",
                "-b:a",
                "128k",
                str(out),
            ]
            subprocess.run(re_cmd, check=True)

        if sys.platform == "darwin":
            subprocess.Popen(["open", str(out)])
        elif os.name == "nt":
            os.startfile(str(out))
        else:
            subprocess.Popen(["xdg-open", str(out)])
        return True


def save_sequence(paths, output_path: Path):
    if not paths:
        return

    ffmpeg = shutil.which("ffmpeg") or "ffmpeg"

    with tempfile.NamedTemporaryFile(
        "w", suffix=".txt", delete=False, encoding="utf-8"
    ) as f:
        lst_path = f.name
        for p in paths:
            safe_path = str(Path(p).resolve()).replace("\\", "/")
            f.write(f"file '{safe_path}'\n")

    try:
        cmd = [
            ffmpeg,
            "-y",
            "-f",
            "concat",
            "-safe",
            "0",
            "-i",
            lst_path,
            "-c:v",
            "libx264",
            "-pix_fmt",
            "yuv420p",
            "-c:a",
            "aac",
            "-loglevel",
            "error",
            str(output_path),
        ]
        subprocess.run(cmd, check=True)
        print(f"ğŸ’¾ ì˜ìƒ ì €ì¥ ì™„ë£Œ: {output_path}")
    except Exception as e:
        print(f"âŒ ì˜ìƒ ì €ì¥ ì‹¤íŒ¨: {e}")
    finally:
        try:
            os.remove(lst_path)
        except Exception:
            pass


# ëª¨ë“ˆ ë¡œë“œ ì‹œ Gemini ëª¨ë¸ í•œ ë²ˆë§Œ ë¹Œë“œ
if GOOGLE_API_KEY and genai is not None:
    try:
        GEMINI_MODEL = build_gemini()
        print("[Gemini] ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")
    except Exception as e:
        GEMINI_MODEL = None
        print(f"[Gemini] ì´ˆê¸°í™” ì‹¤íŒ¨, ë¡œì»¬ ê·œì¹™ë§Œ ì‚¬ìš©: {e}")
else:
    GEMINI_MODEL = None
    print("[Gemini] API í‚¤ ì—†ìŒ â†’ ë¡œì»¬ ê·œì¹™ë§Œ ì‚¬ìš©")

# ğŸ”¹ Whisper ëª¨ë¸ë„ ì„œë²„ ì‹œì‘ ì‹œ ë¯¸ë¦¬ ë¡œë”©
try:
    _get_whisper_model()
    print("[Whisper] ëª¨ë¸ ë¯¸ë¦¬ ë¡œë”© ì™„ë£Œ")
except Exception as e:
    print(f"[Whisper] ëª¨ë¸ ë¯¸ë¦¬ ë¡œë”© ì‹¤íŒ¨: {e}")


# ======================================================================
# ë¡œì»¬ ê·œì¹™ ê¸°ë°˜ gloss ì¶”ì¶œ (service.pyì—ì„œ Gemini ì‹¤íŒ¨ ì‹œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ìµœì†Œ ë²„ì „)
# ======================================================================
def _local_gloss_rules(text: str) -> list[str]:
    """
    Gemini ì—†ì´ë„ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ì´ˆê°„ë‹¨ í´ë°± ê·œì¹™.
    (extract_tokensì˜ ë¡œì»¬ í´ë°±ê³¼ ë™ì¼ íŒ¨í„´, ëª¨ë‘ gloss ì·¨ê¸‰)
    """
    clean = _norm(text)
    tokens = re.findall(
        r"""
        \d+(?:\.\d+)?(?:ì²œë§Œ|ì²œë§Œì›|ì²œì›|ì²œ)?(?:ì–µ|ì–µì›)?(?:ë§Œ|ë§Œì›)?(?:ì›)?
        |
        \d+(?:\.\d+)?(?:ê°œì›”|ë…„|ì„¸|%|í¼ì„¼íŠ¸|í¬ì¸íŠ¸)?
        |
        [ê°€-í£A-Za-z]+
        """,
        clean,
        re.VERBOSE,
    )
    return [_first_word(t) for t in tokens if _first_word(t)]
